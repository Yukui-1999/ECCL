{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T17:01:14.788623900Z",
     "start_time": "2024-01-20T17:01:12.848241500Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "import model.ECGEncoder as ECGEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T17:07:36.289062800Z",
     "start_time": "2024-01-20T17:07:36.220605600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is test\n"
     ]
    }
   ],
   "source": [
    "print('this is test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T16:51:06.384350Z",
     "start_time": "2024-01-20T16:51:06.346065500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, epochs=400, accum_iter=1, latent_dim=2048, snp_size=(49, 120), use_snp=True, snp_drop_out=0.0, snp_att_depth=12, snp_global_pool=False, ecg_pretrained=True, ecg_model='vit_base_patchX', ecg_pretrained_model='/mnt/data/dingzhengyao/work/checkpoint/ECG_CMR/outputdir/1009/checkpoint-377-ncc-0.96.pth', ecg_input_channels=1, ecg_input_electrodes=12, ecg_time_steps=5000, ecg_input_size=(12, 5000), ecg_patch_height=1, ecg_patch_width=500, ecg_patch_size=(1, 500), ecg_globle_pool=False, ecg_drop_out=0.0, norm_pix_loss=False, cmr_model='vit_base_patch8', cmr_inchannels=10, cmr_pretrained=False, img_size=80, cmr_patch_height=8, cmr_patch_width=8, cmr_drop_out=0.0, tar_pretrained=True, tar_number=195, tar_pretrained_path='/home/dingzhengyao/Work/ECG_CMR/tabnet/pretrain_tabnet_model_by_train_data_1.zip', loss='clip_loss', margin=0.025, temperature=0.1, alpha_weight=0.25, loss_type='ecg_cmr', input_size=(12, 5000), timeFlip=0.33, signFlip=0.33, weight_decay=0.05, lr=None, blr=0.0001, min_lr=0.0, warmup_epochs=40, patience=-1, max_delta=0, data_path='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/train_data_dict_v5.pt', val_data_path='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/val_data_dict_v5.pt', test_data_path='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/test_data_dict_v5.pt', output_dir='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/output_dir', log_dir='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/log_dir', wandb=True, wandb_project='CMR_ECG_TAR', wandb_id='1001', device='cuda:3', seed=0, resume='', start_epoch=0, num_workers=8, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Tuple\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    # Basic parameters\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory '\n",
    "                             'constraints)')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--latent_dim', default=2048, type=int, metavar='N',\n",
    "                        help='latent_dim')\n",
    "    # SNP parameters\n",
    "    parser.add_argument('--snp_size', default=(49, 120), type=Tuple, help='ecg input size')\n",
    "    parser.add_argument('--use_snp', default=True, type=str2bool, help='use_snp')\n",
    "    parser.add_argument('--snp_drop_out', default=0.0, type=float)\n",
    "    parser.add_argument('--snp_att_depth', default=12, type=int)\n",
    "    parser.add_argument('--snp_global_pool', default=False, type=str2bool, help='snp_global_pool')\n",
    "    # ECG Model parameters\n",
    "    parser.add_argument('--ecg_pretrained', default=True, type=str2bool,help='ecg_pretrained or not')\n",
    "    parser.add_argument('--ecg_model', default='vit_base_patchX', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--ecg_pretrained_model',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/ECG_CMR/outputdir/1009/checkpoint-377-ncc-0.96.pth',\n",
    "                        type=str, metavar='MODEL', help='path of pretaained model')\n",
    "    parser.add_argument('--ecg_input_channels', type=int, default=1, metavar='N',\n",
    "                        help='ecginput_channels')\n",
    "    parser.add_argument('--ecg_input_electrodes', type=int, default=12, metavar='N',\n",
    "                        help='ecg input electrodes')\n",
    "    parser.add_argument('--ecg_time_steps', type=int, default=5000, metavar='N',\n",
    "                        help='ecg input length')\n",
    "    parser.add_argument('--ecg_input_size', default=(12, 5000), type=Tuple,\n",
    "                        help='ecg input size')\n",
    "    parser.add_argument('--ecg_patch_height', type=int, default=1, metavar='N',\n",
    "                        help='ecg patch height')\n",
    "    parser.add_argument('--ecg_patch_width', type=int, default=500, metavar='N',\n",
    "                        help='ecg patch width')\n",
    "    parser.add_argument('--ecg_patch_size', default=(1, 500), type=Tuple,\n",
    "                        help='ecg patch size')\n",
    "    parser.add_argument('--ecg_globle_pool', default=False,type=str2bool, help='ecg_globle_pool')\n",
    "    parser.add_argument('--ecg_drop_out', default=0.0, type=float)\n",
    "    parser.add_argument('--norm_pix_loss', action='store_true', default=False,\n",
    "                        help='Use (per-patch) normalized pixels as targets for computing loss')\n",
    "    \n",
    "\n",
    "    # CMR Model parameters\n",
    "    parser.add_argument('--cmr_model', default='vit_base_patch8', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--cmr_inchannels', default=10, type=int, metavar='N',\n",
    "                        help='cmr_inchannels')\n",
    "    parser.add_argument('--cmr_pretrained', default=False, type=str2bool,\n",
    "                        help='cmr_pretrained or not')\n",
    "    parser.add_argument('--img_size', default=80, type=int, metavar='N', help='img_size of cmr')\n",
    "    parser.add_argument('--cmr_patch_height', type=int, default=8, metavar='N',\n",
    "                        help='cmr patch height')\n",
    "    parser.add_argument('--cmr_patch_width', type=int, default=8, metavar='N',\n",
    "                        help='cmr patch width')\n",
    "    parser.add_argument('--cmr_drop_out', default=0.0, type=float)\n",
    "    \n",
    "    # TAR Model parameters\n",
    "    parser.add_argument('--tar_pretrained', default=True, type=str2bool,help='tar_pretrained or not')\n",
    "    parser.add_argument('--tar_number', default=195, type=int, metavar='N',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--tar_pretrained_path',\n",
    "                        default='/home/dingzhengyao/Work/ECG_CMR/tabnet/pretrain_tabnet_model_by_train_data_1.zip',\n",
    "                        type=str, metavar='MODEL', help='path of pretaained model')\n",
    "    \n",
    "\n",
    "    # LOSS parameters\n",
    "    parser.add_argument('--loss', default='clip_loss', type=str, metavar='LOSS', help='loss function')\n",
    "    parser.add_argument('--margin', default=0.025, type=float, metavar='MARGIN', help='margin for triplet loss')\n",
    "    parser.add_argument('--temperature', default=0.1, type=float, metavar='TEMPERATURE',\n",
    "                        help='temperature for nt_xent loss')\n",
    "    parser.add_argument('--alpha_weight', default=0.25, type=float, metavar='ALPHA_WEIGHT',\n",
    "                        help='alpha_weight for nt_xent loss')\n",
    "    parser.add_argument('--loss_type', default='ecg_cmr', type=str, help='loss_type')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--input_size', type=tuple, default=(12, 5000))\n",
    "\n",
    "    parser.add_argument('--timeFlip', type=float, default=0.33)\n",
    "\n",
    "    parser.add_argument('--signFlip', type=float, default=0.33)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--blr', type=float, default=1e-4, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=40, metavar='N',\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Callback parameters\n",
    "    parser.add_argument('--patience', default=-1, type=float,\n",
    "                        help='Early stopping whether val is worse than train for specified nb of epochs (default: -1, i.e. no early stopping)')\n",
    "    parser.add_argument('--max_delta', default=0, type=float,\n",
    "                        help='Early stopping threshold (val has to be worse than (train+delta)) (default: 0)')\n",
    "\n",
    "    # Dataset parameters\n",
    "\n",
    "    parser.add_argument('--data_path',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/train_data_dict_v5.pt',\n",
    "                        type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--val_data_path',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/val_data_dict_v5.pt',\n",
    "                        type=str,\n",
    "                        help='validation dataset path')\n",
    "    parser.add_argument('--test_data_path',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/test_data_dict_v5.pt',\n",
    "                        type=str,\n",
    "                        help='test dataset path')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/output_dir',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/log_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--wandb', type=str2bool,  default=True)\n",
    "    parser.add_argument('--wandb_project', default='CMR_ECG_TAR',\n",
    "                        help='project where to wandb log')\n",
    "    # parser.add_argument('--wandb_dir', default='/mnt/data/dingzhengyao/work/checkpoint/ECG_CMR/wandb/1002',\n",
    "    #                     help='project where to wandb save')\n",
    "    parser.add_argument('--wandb_id', default='1001', type=str,\n",
    "                        help='id of the current run')\n",
    "    parser.add_argument('--device', default='cuda:3',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true', default=True, \n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "\n",
    "    # Distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    return parser\n",
    "\n",
    "args = get_args_parser().parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_tokens.shape: torch.Size([2, 1, 120])\n",
      "x.shape: torch.Size([2, 49, 120])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1              [-1, 50, 120]               0\n",
      "MultiheadAttention-2  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "         LayerNorm-3              [-1, 50, 120]             240\n",
      "           Dropout-4              [-1, 50, 120]               0\n",
      "            Linear-5              [-1, 50, 480]          58,080\n",
      "              ReLU-6              [-1, 50, 480]               0\n",
      "            Linear-7              [-1, 50, 120]          57,720\n",
      "         LayerNorm-8              [-1, 50, 120]             240\n",
      "           Dropout-9              [-1, 50, 120]               0\n",
      " TransformerBlock-10              [-1, 50, 120]               0\n",
      "MultiheadAttention-11  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-12              [-1, 50, 120]             240\n",
      "          Dropout-13              [-1, 50, 120]               0\n",
      "           Linear-14              [-1, 50, 480]          58,080\n",
      "             ReLU-15              [-1, 50, 480]               0\n",
      "           Linear-16              [-1, 50, 120]          57,720\n",
      "        LayerNorm-17              [-1, 50, 120]             240\n",
      "          Dropout-18              [-1, 50, 120]               0\n",
      " TransformerBlock-19              [-1, 50, 120]               0\n",
      "MultiheadAttention-20  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-21              [-1, 50, 120]             240\n",
      "          Dropout-22              [-1, 50, 120]               0\n",
      "           Linear-23              [-1, 50, 480]          58,080\n",
      "             ReLU-24              [-1, 50, 480]               0\n",
      "           Linear-25              [-1, 50, 120]          57,720\n",
      "        LayerNorm-26              [-1, 50, 120]             240\n",
      "          Dropout-27              [-1, 50, 120]               0\n",
      " TransformerBlock-28              [-1, 50, 120]               0\n",
      "MultiheadAttention-29  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-30              [-1, 50, 120]             240\n",
      "          Dropout-31              [-1, 50, 120]               0\n",
      "           Linear-32              [-1, 50, 480]          58,080\n",
      "             ReLU-33              [-1, 50, 480]               0\n",
      "           Linear-34              [-1, 50, 120]          57,720\n",
      "        LayerNorm-35              [-1, 50, 120]             240\n",
      "          Dropout-36              [-1, 50, 120]               0\n",
      " TransformerBlock-37              [-1, 50, 120]               0\n",
      "MultiheadAttention-38  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-39              [-1, 50, 120]             240\n",
      "          Dropout-40              [-1, 50, 120]               0\n",
      "           Linear-41              [-1, 50, 480]          58,080\n",
      "             ReLU-42              [-1, 50, 480]               0\n",
      "           Linear-43              [-1, 50, 120]          57,720\n",
      "        LayerNorm-44              [-1, 50, 120]             240\n",
      "          Dropout-45              [-1, 50, 120]               0\n",
      " TransformerBlock-46              [-1, 50, 120]               0\n",
      "MultiheadAttention-47  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-48              [-1, 50, 120]             240\n",
      "          Dropout-49              [-1, 50, 120]               0\n",
      "           Linear-50              [-1, 50, 480]          58,080\n",
      "             ReLU-51              [-1, 50, 480]               0\n",
      "           Linear-52              [-1, 50, 120]          57,720\n",
      "        LayerNorm-53              [-1, 50, 120]             240\n",
      "          Dropout-54              [-1, 50, 120]               0\n",
      " TransformerBlock-55              [-1, 50, 120]               0\n",
      "MultiheadAttention-56  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-57              [-1, 50, 120]             240\n",
      "          Dropout-58              [-1, 50, 120]               0\n",
      "           Linear-59              [-1, 50, 480]          58,080\n",
      "             ReLU-60              [-1, 50, 480]               0\n",
      "           Linear-61              [-1, 50, 120]          57,720\n",
      "        LayerNorm-62              [-1, 50, 120]             240\n",
      "          Dropout-63              [-1, 50, 120]               0\n",
      " TransformerBlock-64              [-1, 50, 120]               0\n",
      "MultiheadAttention-65  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-66              [-1, 50, 120]             240\n",
      "          Dropout-67              [-1, 50, 120]               0\n",
      "           Linear-68              [-1, 50, 480]          58,080\n",
      "             ReLU-69              [-1, 50, 480]               0\n",
      "           Linear-70              [-1, 50, 120]          57,720\n",
      "        LayerNorm-71              [-1, 50, 120]             240\n",
      "          Dropout-72              [-1, 50, 120]               0\n",
      " TransformerBlock-73              [-1, 50, 120]               0\n",
      "MultiheadAttention-74  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-75              [-1, 50, 120]             240\n",
      "          Dropout-76              [-1, 50, 120]               0\n",
      "           Linear-77              [-1, 50, 480]          58,080\n",
      "             ReLU-78              [-1, 50, 480]               0\n",
      "           Linear-79              [-1, 50, 120]          57,720\n",
      "        LayerNorm-80              [-1, 50, 120]             240\n",
      "          Dropout-81              [-1, 50, 120]               0\n",
      " TransformerBlock-82              [-1, 50, 120]               0\n",
      "MultiheadAttention-83  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-84              [-1, 50, 120]             240\n",
      "          Dropout-85              [-1, 50, 120]               0\n",
      "           Linear-86              [-1, 50, 480]          58,080\n",
      "             ReLU-87              [-1, 50, 480]               0\n",
      "           Linear-88              [-1, 50, 120]          57,720\n",
      "        LayerNorm-89              [-1, 50, 120]             240\n",
      "          Dropout-90              [-1, 50, 120]               0\n",
      " TransformerBlock-91              [-1, 50, 120]               0\n",
      "MultiheadAttention-92  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "        LayerNorm-93              [-1, 50, 120]             240\n",
      "          Dropout-94              [-1, 50, 120]               0\n",
      "           Linear-95              [-1, 50, 480]          58,080\n",
      "             ReLU-96              [-1, 50, 480]               0\n",
      "           Linear-97              [-1, 50, 120]          57,720\n",
      "        LayerNorm-98              [-1, 50, 120]             240\n",
      "          Dropout-99              [-1, 50, 120]               0\n",
      "TransformerBlock-100              [-1, 50, 120]               0\n",
      "MultiheadAttention-101  [[-1, 50, 120], [-1, 50, 50]]               0\n",
      "       LayerNorm-102              [-1, 50, 120]             240\n",
      "         Dropout-103              [-1, 50, 120]               0\n",
      "          Linear-104              [-1, 50, 480]          58,080\n",
      "            ReLU-105              [-1, 50, 480]               0\n",
      "          Linear-106              [-1, 50, 120]          57,720\n",
      "       LayerNorm-107              [-1, 50, 120]             240\n",
      "         Dropout-108              [-1, 50, 120]               0\n",
      "TransformerBlock-109              [-1, 50, 120]               0\n",
      "       LayerNorm-110              [-1, 50, 120]             240\n",
      "          Linear-111                 [-1, 2048]         247,808\n",
      "================================================================\n",
      "Total params: 1,643,408\n",
      "Trainable params: 1,643,408\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 1365.49\n",
      "Params size (MB): 6.27\n",
      "Estimated Total Size (MB): 1371.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.SNPEncoder import SNPEncoder\n",
    "from torchsummary import summary\n",
    "model = SNPEncoder(args)\n",
    "summary(model, input_size=( 49, 120),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T16:52:27.448453300Z",
     "start_time": "2024-01-20T16:52:26.160204100Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'snp_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_new \u001b[38;5;241m=\u001b[39m \u001b[43mECGEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg_new_model\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg_input_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg_patch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg_input_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Work/ECG_CMR/ECG_CMR_TAR/Project_version1/test_code/../model/ECGEncoder.py:151\u001b[0m, in \u001b[0;36mvit_base_patchX\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvit_base_patchX\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 151\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mECGEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Work/ECG_CMR/ECG_CMR_TAR/Project_version1/test_code/../model/ECGEncoder.py:35\u001b[0m, in \u001b[0;36mECGEncoder.__init__\u001b[0;34m(self, global_pool, use_snp, args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_norm \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_snp \u001b[38;5;241m=\u001b[39m use_snp\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnp_fc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnp_size\u001b[49m[\u001b[38;5;241m1\u001b[39m], kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnp_conv1d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv1d(in_channels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msnp_size[\u001b[38;5;241m0\u001b[39m], out_channels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mecg_patch_num, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnp_attention \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m     39\u001b[0m     TransformerBlock(\n\u001b[1;32m     40\u001b[0m         embed_size\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks))\n\u001b[1;32m     45\u001b[0m ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'snp_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_new = ECGEncoder.__dict__[args.ecg_new_model](\n",
    "        img_size=args.ecg_input_size,\n",
    "        patch_size=args.ecg_patch_size,\n",
    "        in_chans=args.ecg_input_channels,\n",
    "        num_classes=args.latent_dim,\n",
    "    )\n",
    "# model_new = vit_base_patch200(patch_size=(1,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedAutoencoderViT(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(1, 500), stride=(1, 500))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_pred): Linear(in_features=512, out_features=500, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGEncoder(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(1, 500), stride=(1, 500))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Linear(in_features=768, out_features=2048, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: cls_token\n",
      "Parameter name: pos_embed\n",
      "Parameter name: patch_embed.proj.weight\n",
      "Parameter name: patch_embed.proj.bias\n",
      "Parameter name: blocks.0.norm1.weight\n",
      "Parameter name: blocks.0.norm1.bias\n",
      "Parameter name: blocks.0.attn.qkv.weight\n",
      "Parameter name: blocks.0.attn.qkv.bias\n",
      "Parameter name: blocks.0.attn.proj.weight\n",
      "Parameter name: blocks.0.attn.proj.bias\n",
      "Parameter name: blocks.0.norm2.weight\n",
      "Parameter name: blocks.0.norm2.bias\n",
      "Parameter name: blocks.0.mlp.fc1.weight\n",
      "Parameter name: blocks.0.mlp.fc1.bias\n",
      "Parameter name: blocks.0.mlp.fc2.weight\n",
      "Parameter name: blocks.0.mlp.fc2.bias\n",
      "Parameter name: blocks.1.norm1.weight\n",
      "Parameter name: blocks.1.norm1.bias\n",
      "Parameter name: blocks.1.attn.qkv.weight\n",
      "Parameter name: blocks.1.attn.qkv.bias\n",
      "Parameter name: blocks.1.attn.proj.weight\n",
      "Parameter name: blocks.1.attn.proj.bias\n",
      "Parameter name: blocks.1.norm2.weight\n",
      "Parameter name: blocks.1.norm2.bias\n",
      "Parameter name: blocks.1.mlp.fc1.weight\n",
      "Parameter name: blocks.1.mlp.fc1.bias\n",
      "Parameter name: blocks.1.mlp.fc2.weight\n",
      "Parameter name: blocks.1.mlp.fc2.bias\n",
      "Parameter name: blocks.2.norm1.weight\n",
      "Parameter name: blocks.2.norm1.bias\n",
      "Parameter name: blocks.2.attn.qkv.weight\n",
      "Parameter name: blocks.2.attn.qkv.bias\n",
      "Parameter name: blocks.2.attn.proj.weight\n",
      "Parameter name: blocks.2.attn.proj.bias\n",
      "Parameter name: blocks.2.norm2.weight\n",
      "Parameter name: blocks.2.norm2.bias\n",
      "Parameter name: blocks.2.mlp.fc1.weight\n",
      "Parameter name: blocks.2.mlp.fc1.bias\n",
      "Parameter name: blocks.2.mlp.fc2.weight\n",
      "Parameter name: blocks.2.mlp.fc2.bias\n",
      "Parameter name: blocks.3.norm1.weight\n",
      "Parameter name: blocks.3.norm1.bias\n",
      "Parameter name: blocks.3.attn.qkv.weight\n",
      "Parameter name: blocks.3.attn.qkv.bias\n",
      "Parameter name: blocks.3.attn.proj.weight\n",
      "Parameter name: blocks.3.attn.proj.bias\n",
      "Parameter name: blocks.3.norm2.weight\n",
      "Parameter name: blocks.3.norm2.bias\n",
      "Parameter name: blocks.3.mlp.fc1.weight\n",
      "Parameter name: blocks.3.mlp.fc1.bias\n",
      "Parameter name: blocks.3.mlp.fc2.weight\n",
      "Parameter name: blocks.3.mlp.fc2.bias\n",
      "Parameter name: blocks.4.norm1.weight\n",
      "Parameter name: blocks.4.norm1.bias\n",
      "Parameter name: blocks.4.attn.qkv.weight\n",
      "Parameter name: blocks.4.attn.qkv.bias\n",
      "Parameter name: blocks.4.attn.proj.weight\n",
      "Parameter name: blocks.4.attn.proj.bias\n",
      "Parameter name: blocks.4.norm2.weight\n",
      "Parameter name: blocks.4.norm2.bias\n",
      "Parameter name: blocks.4.mlp.fc1.weight\n",
      "Parameter name: blocks.4.mlp.fc1.bias\n",
      "Parameter name: blocks.4.mlp.fc2.weight\n",
      "Parameter name: blocks.4.mlp.fc2.bias\n",
      "Parameter name: blocks.5.norm1.weight\n",
      "Parameter name: blocks.5.norm1.bias\n",
      "Parameter name: blocks.5.attn.qkv.weight\n",
      "Parameter name: blocks.5.attn.qkv.bias\n",
      "Parameter name: blocks.5.attn.proj.weight\n",
      "Parameter name: blocks.5.attn.proj.bias\n",
      "Parameter name: blocks.5.norm2.weight\n",
      "Parameter name: blocks.5.norm2.bias\n",
      "Parameter name: blocks.5.mlp.fc1.weight\n",
      "Parameter name: blocks.5.mlp.fc1.bias\n",
      "Parameter name: blocks.5.mlp.fc2.weight\n",
      "Parameter name: blocks.5.mlp.fc2.bias\n",
      "Parameter name: blocks.6.norm1.weight\n",
      "Parameter name: blocks.6.norm1.bias\n",
      "Parameter name: blocks.6.attn.qkv.weight\n",
      "Parameter name: blocks.6.attn.qkv.bias\n",
      "Parameter name: blocks.6.attn.proj.weight\n",
      "Parameter name: blocks.6.attn.proj.bias\n",
      "Parameter name: blocks.6.norm2.weight\n",
      "Parameter name: blocks.6.norm2.bias\n",
      "Parameter name: blocks.6.mlp.fc1.weight\n",
      "Parameter name: blocks.6.mlp.fc1.bias\n",
      "Parameter name: blocks.6.mlp.fc2.weight\n",
      "Parameter name: blocks.6.mlp.fc2.bias\n",
      "Parameter name: blocks.7.norm1.weight\n",
      "Parameter name: blocks.7.norm1.bias\n",
      "Parameter name: blocks.7.attn.qkv.weight\n",
      "Parameter name: blocks.7.attn.qkv.bias\n",
      "Parameter name: blocks.7.attn.proj.weight\n",
      "Parameter name: blocks.7.attn.proj.bias\n",
      "Parameter name: blocks.7.norm2.weight\n",
      "Parameter name: blocks.7.norm2.bias\n",
      "Parameter name: blocks.7.mlp.fc1.weight\n",
      "Parameter name: blocks.7.mlp.fc1.bias\n",
      "Parameter name: blocks.7.mlp.fc2.weight\n",
      "Parameter name: blocks.7.mlp.fc2.bias\n",
      "Parameter name: blocks.8.norm1.weight\n",
      "Parameter name: blocks.8.norm1.bias\n",
      "Parameter name: blocks.8.attn.qkv.weight\n",
      "Parameter name: blocks.8.attn.qkv.bias\n",
      "Parameter name: blocks.8.attn.proj.weight\n",
      "Parameter name: blocks.8.attn.proj.bias\n",
      "Parameter name: blocks.8.norm2.weight\n",
      "Parameter name: blocks.8.norm2.bias\n",
      "Parameter name: blocks.8.mlp.fc1.weight\n",
      "Parameter name: blocks.8.mlp.fc1.bias\n",
      "Parameter name: blocks.8.mlp.fc2.weight\n",
      "Parameter name: blocks.8.mlp.fc2.bias\n",
      "Parameter name: blocks.9.norm1.weight\n",
      "Parameter name: blocks.9.norm1.bias\n",
      "Parameter name: blocks.9.attn.qkv.weight\n",
      "Parameter name: blocks.9.attn.qkv.bias\n",
      "Parameter name: blocks.9.attn.proj.weight\n",
      "Parameter name: blocks.9.attn.proj.bias\n",
      "Parameter name: blocks.9.norm2.weight\n",
      "Parameter name: blocks.9.norm2.bias\n",
      "Parameter name: blocks.9.mlp.fc1.weight\n",
      "Parameter name: blocks.9.mlp.fc1.bias\n",
      "Parameter name: blocks.9.mlp.fc2.weight\n",
      "Parameter name: blocks.9.mlp.fc2.bias\n",
      "Parameter name: blocks.10.norm1.weight\n",
      "Parameter name: blocks.10.norm1.bias\n",
      "Parameter name: blocks.10.attn.qkv.weight\n",
      "Parameter name: blocks.10.attn.qkv.bias\n",
      "Parameter name: blocks.10.attn.proj.weight\n",
      "Parameter name: blocks.10.attn.proj.bias\n",
      "Parameter name: blocks.10.norm2.weight\n",
      "Parameter name: blocks.10.norm2.bias\n",
      "Parameter name: blocks.10.mlp.fc1.weight\n",
      "Parameter name: blocks.10.mlp.fc1.bias\n",
      "Parameter name: blocks.10.mlp.fc2.weight\n",
      "Parameter name: blocks.10.mlp.fc2.bias\n",
      "Parameter name: blocks.11.norm1.weight\n",
      "Parameter name: blocks.11.norm1.bias\n",
      "Parameter name: blocks.11.attn.qkv.weight\n",
      "Parameter name: blocks.11.attn.qkv.bias\n",
      "Parameter name: blocks.11.attn.proj.weight\n",
      "Parameter name: blocks.11.attn.proj.bias\n",
      "Parameter name: blocks.11.norm2.weight\n",
      "Parameter name: blocks.11.norm2.bias\n",
      "Parameter name: blocks.11.mlp.fc1.weight\n",
      "Parameter name: blocks.11.mlp.fc1.bias\n",
      "Parameter name: blocks.11.mlp.fc2.weight\n",
      "Parameter name: blocks.11.mlp.fc2.bias\n",
      "Parameter name: norm.weight\n",
      "Parameter name: norm.bias\n",
      "Parameter name: head.weight\n",
      "Parameter name: head.bias\n"
     ]
    }
   ],
   "source": [
    "state_dict = model_new.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    print(f'Parameter name: {name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/mnt/data/dingzhengyao/work/checkpoint/ECG_CMR/outputdir/1009/checkpoint-377-ncc-0.96.pth', map_location='cpu')\n",
    "checkpoint_model = checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n"
     ]
    }
   ],
   "source": [
    "msg = model_new.load_state_dict(checkpoint_model, strict=False)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['cls_token', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 12, 5000])\n"
     ]
    }
   ],
   "source": [
    "loss, imgs_hat, imgs_hat_masked = model_old(torch.randn(2,1, 12, 5000))\n",
    "print(imgs_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2048])\n"
     ]
    }
   ],
   "source": [
    "res = model_new(torch.randn(2,1, 12, 5000))\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model.resnet as resnet\n",
    "# CMR_encoder = resnet.__dict__[args.cmr_model](\n",
    "#             in_channels = args.cmr_inchannels,\n",
    "#             latent_dim = args.latent_dim,\n",
    "#             pretrained = args.cmr_pretrained,\n",
    "#         )\n",
    "from model.resnet  import resnet18\n",
    "CMR_encoder=resnet18(in_channels = args.cmr_inchannels,latent_dim=args.latent_dim,pretrained=args.cmr_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = CMR_encoder(torch.randn(2,10, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.nt_xent import NTXentLoss\n",
    "from loss.triplet import TripletLoss\n",
    "ecg = torch.randn(2,2048).to(args.device)\n",
    "cmr = torch.randn(2,2048).to(args.device)\n",
    "tar = torch.randn(2,2048).to(args.device)\n",
    "loss_func_ntx = NTXentLoss(temperature=0.1,alpha_weight=0.25,args=args ).to(args.device)\n",
    "loss_func_triplet = TripletLoss(margin=0.025).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ecg_feature', 'cmr_feature')\n",
      "('ecg_feature', 'tar_feature')\n",
      "('cmr_feature', 'tar_feature')\n",
      "{'train_loss/ecg_cmr_loss': tensor(0.5041, device='cuda:3'), 'train_loss/ecg_tar_loss': tensor(0.6826, device='cuda:3'), 'train_loss/cmr_tar_loss': tensor(0.8210, device='cuda:3'), 'train_loss/total_loss': tensor(2.0077, device='cuda:3')}\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "output_dict = {'ecg_feature':ecg,'cmr_feature':cmr,'tar_feature':tar}\n",
    "all_combinations = combinations(output_dict.keys(), 2)\n",
    "\n",
    "loss_dict = {}\n",
    "loss_prefix = 'train_loss'\n",
    "for key_combination in all_combinations:\n",
    "    print(key_combination)\n",
    "    loss_name = f\"{loss_prefix}/{key_combination[0][:-8]}_{key_combination[1][:-8]}_loss\"\n",
    "    loss_dict[loss_name] = loss_func_ntx(output_dict[key_combination[0]], output_dict[key_combination[1]])\n",
    "loss_dict[f\"{loss_prefix}/total_loss\"] = sum(loss_dict.values())\n",
    "print(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np\n",
    "\n",
    "def get_ecg(ecg_path):\n",
    "    ecg_file = open(ecg_path).read()\n",
    "    bs = None\n",
    "    try:\n",
    "        bs = BeautifulSoup(ecg_file, features=\"lxml\")\n",
    "    except:\n",
    "        pass\n",
    "    ecg_waveform_length = 5000\n",
    "    if ecg_waveform_length == 600:\n",
    "        waveform = bs.body.cardiologyxml.mediansamples\n",
    "    else:\n",
    "        waveform = bs.body.cardiologyxml.stripdata\n",
    "    # print(waveform)\n",
    "    # print(type(waveform))\n",
    "    data_numpy = None\n",
    "    bs_measurement = bs.body.cardiologyxml.restingecgmeasurements\n",
    "    heartbeat = int(bs_measurement.find_all(\"VentricularRate\".lower())[0].string)\n",
    "\n",
    "    for each_wave in waveform.find_all(\"waveformdata\"):\n",
    "        each_data = each_wave.string.strip().split(\",\")\n",
    "        each_data = [s.replace('\\n\\t\\t', '') for s in each_data]\n",
    "        each_data = np.array(each_data, dtype=np.float32)\n",
    "        # plt.plot(each_data)\n",
    "        try:\n",
    "            seasonal_decompose_result = seasonal_decompose(each_data, model=\"additive\",\n",
    "                                                        period=int(ecg_waveform_length*6/heartbeat))\n",
    "        except:\n",
    "            return None\n",
    "        trend = seasonal_decompose_result.trend\n",
    "        start, end = 0, ecg_waveform_length - 1\n",
    "        sflag, eflag = False, False\n",
    "        for i in range(ecg_waveform_length):\n",
    "            if np.isnan(trend[i]):\n",
    "                start += 1\n",
    "            else:\n",
    "                sflag = True\n",
    "            if np.isnan(trend[ecg_waveform_length-1-i]):\n",
    "                end -= 1\n",
    "            else:\n",
    "                eflag = True\n",
    "            if sflag and eflag:\n",
    "                break\n",
    "        trend[:start] = trend[start]\n",
    "        trend[end:] = trend[end]\n",
    "        # trend[np.isnan(trend)] = 0.0\n",
    "        result = np.array(seasonal_decompose_result.observed - trend)\n",
    "        # plt.plot(result)\n",
    "        # plt.show()\n",
    "        # exit()\n",
    "        if data_numpy is None:\n",
    "            data_numpy = result\n",
    "        else:\n",
    "            data_numpy = np.vstack((data_numpy, result))\n",
    "\n",
    "\n",
    "    for channel in data_numpy:\n",
    "        # print(channel.shape)\n",
    "        unique_values = np.unique(channel)\n",
    "        if len(unique_values) == 1: \n",
    "            return None\n",
    "    \n",
    "    return data_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8309,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26588/741601615.py:14: DtypeWarning: Columns (176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv = pd.read_csv('/mnt/data/ukb_collation/ukb_ecg_cmr/data/test_v3.csv')\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg 1109736_20205_2_0.xml is None\n",
      "['1109736_20205_2_0.xml']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_ecg(i):\n",
    "    res = get_ecg(os.path.join('/mnt/data/ukb_heartmri/ukb_20205',ecg[i]))\n",
    "    if res is None:\n",
    "        print(f'ecg {ecg[i]} is None')\n",
    "        return ecg[i]\n",
    "    # print(f'Finish processing ecg {ecg[i]}')\n",
    "    return None\n",
    "\n",
    "csv = pd.read_csv('/mnt/data/ukb_collation/ukb_ecg_cmr/data/test_v3.csv')\n",
    "ecg = csv['20205_2_0'].values\n",
    "print(ecg.shape)\n",
    "\n",
    "# 创建一个进程池，进程数等于你的 CPU 核心数\n",
    "with Pool(os.cpu_count()) as p:\n",
    "    non = p.map(process_ecg, range(ecg.shape[0]))\n",
    "\n",
    "# 过滤掉 None 值\n",
    "non = [i for i in non if i is not None]\n",
    "\n",
    "print(non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24918,)\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "175\n",
      "200\n",
      "300\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "# /mnt/data/ukb_heartmri/ukb_20205/2652758_20205_2_0.xml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "csv = pd.read_csv('/mnt/data/ukb_collation/ukb_ecg_cmr/data/train_v3.csv')\n",
    "ecg = csv['20205_2_0'].values\n",
    "print(ecg.shape)\n",
    "non=[]\n",
    "for i in range(ecg.shape[0]):\n",
    "    res = get_ecg(os.path.join('/mnt/data/ukb_heartmri/ukb_20205',ecg[i]))\n",
    "    # print(res.shape)\n",
    "    if i % 100 ==0:\n",
    "        print(i)\n",
    "    if res is None:\n",
    "        non.append(ecg[i])\n",
    "        print(ecg[i])\n",
    "        \n",
    "print(non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.3446), tensor(-2.3446), True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重写和运行上述代码\n",
    "import torch\n",
    "\n",
    "def softXEnt(target, logits):\n",
    "    logprobs = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "    loss = -(target * logprobs).sum() / logits.shape[0]\n",
    "    return loss\n",
    "\n",
    "# 创建一个 4x4 的 target 矩阵和 4x4 的单位矩阵作为 logits\n",
    "target = torch.randn(4, 4)\n",
    "logits = torch.eye(4)\n",
    "\n",
    "# 计算使用原始 target 和 logits 的 softXEnt\n",
    "loss_original = softXEnt(target, logits)\n",
    "\n",
    "# 计算使用 target 的转置和 logits 的 softXEnt\n",
    "loss_transposed = softXEnt(target.t(), logits)\n",
    "\n",
    "loss_original, loss_transposed, torch.equal(loss_original, loss_transposed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14., 77.])\n",
      "tensor([194., 365.])\n",
      "tensor([[14., 77.]])\n",
      "tensor([[194.],\n",
      "        [365.]])\n",
      "tensor([[ 50.,  68.],\n",
      "        [122., 167.]])\n",
      "tensor([[108., 135.],\n",
      "        [135., 108.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设我们有两组一维向量\n",
    "a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = torch.tensor([[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]])\n",
    "\n",
    "# 计算每组向量的平方范数\n",
    "a_square_norm = (a ** 2).sum(dim=1)\n",
    "b_square_norm = (b ** 2).sum(dim=1)\n",
    "print(a_square_norm)\n",
    "print(b_square_norm)\n",
    "print(a_square_norm.unsqueeze(0))\n",
    "print(b_square_norm.unsqueeze(1))\n",
    "# 计算两组向量的点积\n",
    "dot_product = torch.matmul(a, b.t())\n",
    "print(dot_product)\n",
    "\n",
    "# 计算欧氏距离的平方\n",
    "distances = a_square_norm.unsqueeze(0) - 2.0 * dot_product + b_square_norm.unsqueeze(1)\n",
    "\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[15.5885, 10.3923,  5.1962],\n",
       "          [20.7846, 15.5885, 10.3923],\n",
       "          [25.9808, 20.7846, 15.5885]]]),\n",
       " tensor([[243., 432., 675.],\n",
       "         [108., 243., 432.],\n",
       "         [ 27., 108., 243.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入torch库\n",
    "import torch\n",
    "\n",
    "# 定义两个张量a和b\n",
    "a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0],[7.0, 8.0, 9.0]])\n",
    "b = torch.tensor([[10.0, 11.0, 12.0], [13.0, 14.0, 15.0],[16.0, 17.0, 18.0]])\n",
    "\n",
    "# 计算欧氏距离\n",
    "# 方法一：使用torch中的函数\n",
    "distances = torch.cdist(b.unsqueeze(0), a.unsqueeze(0), p=2)\n",
    "\n",
    "# 方法二：手动计算\n",
    "# 扩展a和b以便于广播，然后计算差值\n",
    "diff = a.unsqueeze(1) - b.unsqueeze(0)\n",
    "# 计算差值的平方和，然后取平方根得到欧氏距离\n",
    "euclidean_distances = torch.sum(diff ** 2, dim=2)\n",
    "\n",
    "distances, euclidean_distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "col = [\n",
    "    \"rs79534072_A\", \"rs643420_T\", \"rs12404144_C\", \"rs2009594_A\", \"rs7354918_G\", \"rs650720_T\", \"rs3738685_T\", \"rs12724121_A\", \"rs934012_A\", \"rs7255_T\", \"rs3813243_T\", \"rs12988307_C\", \"rs2562845_C\", \"rs2042995_C\", \"rs17076_G\", \"rs142556838_T\", \"rs1873164_G\", \"rs55844607_G\", \"rs10497529_A\", \"rs55834511_C\", \"rs9856926_A\", \"rs744892_A\", \"rs6809328_C\", \"rs2686630_C\", \"rs57078287_G\", \"rs6549251_T\", \"rs62253179_A\", \"rs62253185_A\", \"rs55914222_C\", \"rs9850919_C\", \"rs698099_G\", \"rs67846163_G\", \"rs2968210_C\", \"rs154455_T\", \"rs10043782_T\", \"rs55745974_T\", \"rs10065122_C\", \"rs2438150_C\", \"rs72787559_T\", \"rs335196_A\", \"rs7702622_T\", \"rs434775_T\", \"rs72801474_A\", \"rs11745702_C\", \"rs13165478_A\", \"rs1630736_T\", \"rs7744333_C\", \"rs730506_C\", \"rs4151702_C\", \"rs4707174_C\", \"rs7752142_A\", \"rs9401921_G\", \"rs2328474_T\", \"rs13203975_A\", \"rs58127685_T\", \"rs2107595_A\", \"rs336284_A\", \"rs741408_T\", \"rs150260620_A\", \"rs13234515_T\", \"rs4078435_C\", \"rs6974735_G\", \"rs11768878_G\", \"rs11761337_A\", \"rs1583081_T\", \"rs7786419_A\", \"rs2307036_A\", \"rs1915986_A\", \"rs3789849_C\", \"rs907183_C\", \"rs4840467_A\", \"rs7832708_T\", \"rs6601450_T\", \"rs12541800_G\", \"rs11250162_T\", \"rs7823808_C\", \"rs7009229_C\", \"rs34557926_T\", \"rs13252512_G\", \"rs34866937_A\", \"rs11786896_T\", \"rs10740811_G\", \"rs10763764_A\", \"rs2893923_T\", \"rs1896995_T\", \"rs11593126_G\", \"rs2797983_G\", \"rs1343094_T\", \"rs12217597_C\", \"rs7904979_G\", \"rs10885378_C\", \"rs12241957_C\", \"rs7921223_C\", \"rs117550412_T\", \"rs17617337_T\", \"rs72842211_T\", \"rs621679_A\", \"rs78777726_C\", \"rs12285933_T\", \"rs11604825_T\", \"rs11039348_A\", \"rs72931764_A\", \"rs875107_C\", \"rs747249_A\", \"rs861202_G\", \"rs4148674_C\", \"rs73139037_T\", \"rs73145172_T\", \"rs7299436_G\", \"rs597808_A\", \"rs653178_C\", \"rs3914956_T\", \"rs7994761_G\", \"rs376439_G\", \"rs2284651_C\", \"rs61991200_G\", \"rs4905134_A\", \"rs11844114_T\", \"rs17352842_T\", \"rs1561207_T\", \"rs627634_T\", \"rs1441358_G\", \"rs1048661_T\", \"rs12905223_C\", \"rs11638445_A\", \"rs11633377_G\", \"rs11073716_T\", \"rs12595786_C\", \"rs35630683_C\", \"rs72630465_T\", \"rs56864281_A\", \"rs8039472_A\", \"rs35808647_A\", \"rs3803405_A\", \"rs7166287_C\", \"rs77870048_T\", \"rs62053262_G\", \"rs7500448_G\", \"rs488327_C\", \"rs511893_G\", \"rs2126202_C\", \"rs4791494_G\", \"rs12453217_T\", \"rs61572747_G\", \"rs55938136_G\", \"rs242562_A\", \"rs2696532_G\", \"rs199470_C\", \"rs1563304_T\", \"rs17608766_C\", \"rs617759_T\", \"rs59945167_T\", \"rs2070458_A\", \"rs2267038_G\", \"rs133885_A\", \"rs4820654_G\", \"rs57774511_C\"\n",
    "    ]\n",
    "print(len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.ECGEncoder as ECGEncoder\n",
    "model = ECGEncoder.__dict__[args.ecg_new_model](\n",
    "        img_size=args.ecg_input_size,\n",
    "        patch_size=args.ecg_patch_size,\n",
    "        in_chans=args.ecg_input_channels,\n",
    "        num_classes=args.latent_dim,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "x.shape:torch.Size([2, 121, 768])\n",
      "torch.Size([2, 2048])\n"
     ]
    }
   ],
   "source": [
    "res = model(torch.randn(2,1, 12, 5000))\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_dict = {'AAo_aortic_distensibility': [36], \n",
    "                        'AAo_max_area': [84, 89, 101, 102, 103, 105, 112, 116, 120, 121, 136, 137, 139, 141, 140, 156, 8, 23, 22, 28, 31, 33, 35, 37, 39, 40, 45, 52, 53, 56, 57, 58, 61, 63, 64, 69, 71, 74, 76, 77], \n",
    "                        'AAo_min_area': [83, 88, 101, 102, 103, 104, 106, 112, 116, 119, 121, 136, 138, 141, 140, 156, 8, 23, 22, 21, 28, 31, 33, 34, 35, 37, 38, 41, 45, 46, 52, 53, 56, 60, 62, 63, 64, 70, 73, 77],\n",
    "                        'DAo_aortic_distensibility': [87, 78], \n",
    "                        'DAo_max_area': [82, 86, 113, 117, 118, 122, 151, 5, 9, 30, 49, 55],\n",
    "                        'DAo_min_area': [81, 85, 86, 117, 118, 122, 135, 151, 5, 9, 30, 49, 54, 55],\n",
    "                        'Ecc_AHA_1': [11], \n",
    "                        'Ecc_AHA_10': [14], \n",
    "                        'Ecc_AHA_13': [75], \n",
    "                        'Ecc_AHA_15': [65], \n",
    "                        'Ecc_AHA_2': [68, 72], \n",
    "                        'Ecc_AHA_5': [95, 12], \n",
    "                        'Ecc_AHA_8': [32], \n",
    "                        'Ecc_global': [94, 1, 11], \n",
    "                        'Ell_3': [149], \n",
    "                        'Ell_5': [7], \n",
    "                        'Ell_6': [0], \n",
    "                        'Err_AHA_1': [111, 44], \n",
    "                        'Err_AHA_10': [15], \n",
    "                        'Err_AHA_11': [17], \n",
    "                        'Err_AHA_5': [107], \n",
    "                        'Err_AHA_6': [59], \n",
    "                        'Err_global': [90], \n",
    "                        'LAEF': [154], \n",
    "                        'LVEDV': [92, 93, 110, 11, 16, 18, 50], \n",
    "                        'LVEF': [94, 2, 12, 79], \n",
    "                        'LVESV': [91, 94, 3, 153, 11, 16, 18, 66, 79], \n",
    "                        'LVM': [96, 124, 11, 18], \n",
    "                        'LVSV': [50], \n",
    "                        'RVEDV': [109, 149, 13], \n",
    "                        'RVEF': [94, 6, 20, 67, 80], \n",
    "                        'RVESV': [91, 94, 110, 149, 6, 10, 13, 19], \n",
    "                        'RVSV': [109, 114], \n",
    "                        'WT_AHA_1': [123, 48], \n",
    "                        'WT_AHA_10': [133, 51], \n",
    "                        'WT_AHA_11': [133, 142, 144, 145, 152, 24, 47], \n",
    "                        'WT_AHA_12': [129, 133, 134, 144, 147, 26, 47], \n",
    "                        'WT_AHA_13': [115, 125, 146], \n",
    "                        'WT_AHA_14': [128, 147], \n",
    "                        'WT_AHA_15': [133, 4], \n",
    "                        'WT_AHA_16': [132, 144, 147, 4], \n",
    "                        'WT_AHA_2': [48], \n",
    "                        'WT_AHA_3': [127], \n",
    "                        'WT_AHA_5': [97, 126, 155, 25], \n",
    "                        'WT_AHA_6': [131, 25], \n",
    "                        'WT_AHA_7': [99, 129, 130], \n",
    "                        'WT_AHA_8': [100, 131, 150, 29, 42], \n",
    "                        'WT_AHA_9': [143, 43], \n",
    "                        'WT_global': [98, 99, 108, 128, 143, 144, 148, 27, 48]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 49, 120])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.randn(8000, 471)\n",
    "selected_data = []\n",
    "for key in snp_dict.keys():\n",
    "    while len(snp_dict[key]) < 40:\n",
    "        snp_dict[key] = snp_dict[key] + snp_dict[key]\n",
    "    snp_dict[key] = snp_dict[key][:40]\n",
    "    snp_dict[key] = [i*3+j for i in snp_dict[key] for j in range(3)]\n",
    "    index_tensor = torch.tensor(snp_dict[key])\n",
    "    selected_data.append(tensor[:, index_tensor]) \n",
    "selected_data = torch.stack(selected_data, dim=1)\n",
    "print(selected_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "for key in snp_dict.keys():\n",
    "    print(len(snp_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "[36, 84, 89, 101, 102, 103, 105, 112, 116, 120, 121, 136, 137, 139, 141, 140, 156, 8, 23, 22, 28, 31, 33, 35, 37, 39, 40, 45, 52, 53, 56, 57, 58, 61, 63, 64, 69, 71, 74, 76, 77, 83, 88, 101, 102, 103, 104, 106, 112, 116, 119, 121, 136, 138, 141, 140, 156, 8, 23, 22, 21, 28, 31, 33, 34, 35, 37, 38, 41, 45, 46, 52, 53, 56, 60, 62, 63, 64, 70, 73, 77, 87, 78, 82, 86, 113, 117, 118, 122, 151, 5, 9, 30, 49, 55, 81, 85, 86, 117, 118, 122, 135, 151, 5, 9, 30, 49, 54, 55, 11, 14, 75, 65, 68, 72, 95, 12, 32, 94, 1, 11, 149, 7, 0, 111, 44, 15, 17, 107, 59, 90, 154, 92, 93, 110, 11, 16, 18, 50, 94, 2, 12, 79, 91, 94, 3, 153, 11, 16, 18, 66, 79, 96, 124, 11, 18, 50, 109, 149, 13, 94, 6, 20, 67, 80, 91, 94, 110, 149, 6, 10, 13, 19, 109, 114, 123, 48, 133, 51, 133, 142, 144, 145, 152, 24, 47, 129, 133, 134, 144, 147, 26, 47, 115, 125, 146, 128, 147, 133, 4, 132, 144, 147, 4, 48, 127, 97, 126, 155, 25, 131, 25, 99, 129, 130, 100, 131, 150, 29, 42, 143, 43, 98, 99, 108, 128, 143, 144, 148, 27, 48]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "la = []\n",
    "print(len(snp_dict.keys()))\n",
    "for key in snp_dict.keys():\n",
    "    la.append(snp_dict[key])\n",
    "flat_list = sum(la, [])\n",
    "print(flat_list)\n",
    "flat_list.sort()\n",
    "my_set = set(flat_list)\n",
    "my_list = list(my_set)\n",
    "print(my_list)\n",
    "print(len(my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len([84, 89, 101, 102, 103, 105, 112, 116, 120, 121, 136, 137, 139, 141, 140, 156, 8, 23, 22, 28, 31, 33, 35, 37, 39, 40, 45, 52, 53, 56, 57, 58, 61, 63, 64, 69, 71, 74, 76, 77]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs: {'patch_size': 8, 'embed_dim': 768, 'depth': 12, 'num_heads': 12, 'mlp_ratio': 4, 'qkv_bias': True, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'in_chans': 10, 'img_size': 80, 'num_classes': 2048, 'drop_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import model.CMREncoder as CMREncoder\n",
    "model = CMREncoder.__dict__['vit_base_patch8'](\n",
    "            in_chans = 10,\n",
    "            img_size = 80,\n",
    "            num_classes = 2048,\n",
    "            drop_rate = 0.0,\n",
    "            use_snp = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "x.shape: torch.Size([2, 101, 768])\n",
      "torch.Size([2, 2048])\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(2,10, 80, 80)\n",
    "snp = torch.randn(2,49,120)\n",
    "res = model(x,snp)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 49, 768])\n",
      "torch.Size([2, 121, 768])\n",
      "attn_output.shape: torch.Size([2, 121, 768]), attn_output_weights.shape: torch.Size([2, 121, 121])\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.MultiheadAttention(768, 8,batch_first=True)\n",
    "query = torch.randn(2, 49, 120)\n",
    "fc = torch.nn.Linear(120, 768)\n",
    "query = fc(query)\n",
    "print(query.shape)\n",
    "cnn1d = torch.nn.Conv1d(in_channels=49, out_channels=121, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "query = cnn1d(query)\n",
    "print(query.shape)\n",
    "key = torch.randn(2, 121, 768)\n",
    "value = torch.randn(2, 121, 768)\n",
    "attn_output, attn_output_weights = model(query, key, value)\n",
    "print(f'attn_output.shape: {attn_output.shape}, attn_output_weights.shape: {attn_output_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "TransformerBlock(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "torch.Size([32, 10, 256])\n",
      "Sequential(\n",
      "  (0): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (3): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (4): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (5): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (6): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (7): TransformerBlock(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads, dropout=dropout,batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        attention = self.attention(query, key, value, attn_mask=mask)[0]\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "# Example parameters\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "forward_expansion = 4\n",
    "\n",
    "# Example input\n",
    "x = torch.rand(32, 10, embed_size)  # (batch size,sequence length , embedding size)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "output = model(x, x, x)\n",
    "\n",
    "print(output.shape)  # The output shape will be the same as the input shape\n",
    "blocks=[1,2,3,4,5,6,7,8]\n",
    "snp_attention = nn.Sequential(*[\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(len(blocks))\n",
    "        ])\n",
    "for block in snp_attention:\n",
    "    print(block)\n",
    "    output = block(x, x, x)\n",
    "    print(output.shape)\n",
    "print(snp_attention)\n",
    "# output = snp_attention(x, x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_attention = [TransformerBlock(\n",
    "            embed_size=kwargs['embed_dim'],\n",
    "            heads=kwargs['num_heads'],\n",
    "            dropout=kwargs['drop_rate'],\n",
    "            forward_expansion=kwargs['mlp_ratio']) for _ in range(len(self.blocks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T16:53:42.881355700Z",
     "start_time": "2024-01-20T16:53:42.862027400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.1139e-01,  6.9811e-01, -8.8071e-01,  5.0014e-01,  1.6331e+00,\n",
      "            7.4109e-01, -5.9044e-01, -1.3570e+00,  1.5240e+00, -8.8955e-01],\n",
      "          [ 7.0715e-01, -1.9170e+00, -1.3139e+00, -7.1571e-01, -2.1882e+00,\n",
      "           -1.6789e-01, -9.3906e-01,  1.6039e-01,  1.4526e+00,  1.9749e+00],\n",
      "          [ 2.1625e+00, -3.5487e-01,  8.0556e-01,  8.9195e-01,  1.9967e-01,\n",
      "            3.6870e-03, -7.1267e-01, -7.0969e-01,  2.7000e-01, -9.1817e-01],\n",
      "          [-2.0320e+00, -1.0410e+00,  9.6955e-01,  9.2114e-01,  9.0249e-01,\n",
      "           -1.1899e-01, -5.0441e-04,  1.0383e+00, -2.2779e-01,  2.0136e+00],\n",
      "          [ 7.5779e-01, -1.8868e+00,  5.1095e-01, -5.1010e-01, -9.0492e-01,\n",
      "           -1.7090e+00, -1.3796e+00, -8.3750e-01,  8.1200e-01,  4.5537e-01],\n",
      "          [ 8.5657e-01,  2.6001e-01, -1.1027e+00,  6.9889e-01,  9.1474e-01,\n",
      "           -1.8014e+00, -9.9651e-01, -4.6599e-01, -9.4611e-01,  1.0747e+00],\n",
      "          [ 8.2688e-02, -2.1657e-01, -8.7305e-01, -3.1508e-01,  2.0100e+00,\n",
      "           -4.2284e-01,  9.2492e-01,  7.7975e-01, -7.0326e-01, -9.0765e-01],\n",
      "          [ 1.1011e+00, -1.3798e+00,  2.9539e-01,  3.0016e-01, -2.2153e+00,\n",
      "            1.5066e+00, -6.2615e-01, -8.1412e-01, -2.6879e-02, -2.5849e-01],\n",
      "          [-7.7027e-01,  1.2368e+00,  1.0124e+00, -2.3330e-01, -5.7581e-01,\n",
      "           -8.7437e-01, -1.9954e-01,  4.2666e-01,  1.5808e+00,  5.4859e-01],\n",
      "          [-3.6150e-02,  8.6154e-01, -2.4060e-02, -7.5066e-01,  9.0468e-01,\n",
      "           -1.3529e+00, -1.2805e-02,  6.9405e-01, -2.1541e+00,  2.1761e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.randn(1,1,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TabNetPretraining                                            [64, 1024]                --\n",
       "├─EmbeddingGenerator: 1-1                                    [64, 195]                 --\n",
       "├─TabNetEncoder: 1-2                                         [64, 8]                   --\n",
       "│    └─BatchNorm1d: 2-1                                      [64, 195]                 390\n",
       "│    └─FeatTransformer: 2-2                                  [64, 16]                  1,152\n",
       "│    │    └─GLU_Block: 3-1                                   [64, 16]                  6,880\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [64, 16]                  1,152\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [64, 195]                 1,950\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [64, 16]                  8,032\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [64, 195]                 1,950\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [64, 16]                  8,032\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [64, 195]                 1,950\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [64, 16]                  8,032\n",
       "├─TabNetDecoder: 1-3                                         [64, 195]                 --\n",
       "│    └─ModuleList: 2-13                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-21                            [64, 8]                   320\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-23                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-24                            [64, 8]                   320\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-26                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-27                            [64, 8]                   320\n",
       "│    └─Linear: 2-14                                          [64, 195]                 1,560\n",
       "├─Linear: 1-4                                                [64, 1024]                199,680\n",
       "==============================================================================================================\n",
       "Total params: 253,400\n",
       "Trainable params: 253,400\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 15.40\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 1.95\n",
       "Params size (MB): 0.88\n",
       "Estimated Total Size (MB): 2.88\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.tabnet_model import TabNetPretraining, create_group_matrix\n",
    "TAR_encoder = TabNetPretraining(input_dim=195,\n",
    "                                             group_attention_matrix=create_group_matrix([], 195).to('cuda:3'),\n",
    "                                             device='cuda:3', latent_dim=1024)\n",
    "from torchinfo import summary\n",
    "summary(TAR_encoder,input_size=(64,195),device='cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[ 1.0128,  0.1896, -2.4758, -0.7117,  0.5688,  0.9133,  0.3048,  0.0797,\n",
      "          1.7065, -0.9938],\n",
      "        [ 2.1443, -0.0980,  1.2087,  0.7434, -0.5715, -0.8871, -0.5186, -1.1466,\n",
      "         -0.6590,  2.3179],\n",
      "        [ 0.1459, -0.6751,  1.7570, -1.4916, -0.9797,  0.1173, -2.1208,  0.4785,\n",
      "          0.0576, -0.6332],\n",
      "        [-0.4425,  1.5815,  0.1291,  0.0684,  2.2791, -0.2099,  1.9327, -0.1907,\n",
      "         -0.0598,  2.4359],\n",
      "        [-1.0678,  0.8696,  0.6624,  0.5611, -2.5525, -2.4795,  1.2111,  1.8260,\n",
      "          1.6325,  1.6999],\n",
      "        [-0.2327,  0.1760,  0.1092, -0.1660, -0.7890, -1.1932,  0.0429, -1.4152,\n",
      "         -0.6056,  0.2903],\n",
      "        [-0.4656,  1.2410, -0.9214, -0.0034, -0.2849,  0.1114, -0.3553,  2.5984,\n",
      "         -2.0553,  0.0451],\n",
      "        [ 0.2965,  0.2165, -0.2589,  0.0033, -1.5173, -1.6980,  0.2787,  0.3593,\n",
      "          1.2352,  0.5694],\n",
      "        [-0.6082,  0.6363, -0.4313, -2.1708, -1.1455, -0.6580,  0.5761,  1.1367,\n",
      "         -0.7237, -0.3096],\n",
      "        [-0.9097, -0.3177, -1.5193,  0.5826, -1.1161, -1.1302,  0.9495, -2.4260,\n",
      "         -1.3185,  1.0275]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m cross_entropy_1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(tensor1, torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(tensor1)))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 将tensor1转置后与tensor2做cross_entropy\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cross_entropy_2 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m cross_entropy_1, cross_entropy_2\n",
      "File \u001b[0;32m/data/Anaconda3/envs/pl/lib/python3.9/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 生成一个10*10的随机tensor\n",
    "tensor1 = torch.randn(10, 10)\n",
    "\n",
    "# 生成一个10*10的单位矩阵\n",
    "tensor2 = torch.eye(10)\n",
    "print(torch.arange(len(tensor1)))\n",
    "print(tensor1)\n",
    "# 使用torch的cross_entropy计算tensor1和tensor2\n",
    "cross_entropy_1 = F.cross_entropy(tensor1, torch.arange(len(tensor1)))\n",
    "\n",
    "# 将tensor1转置后与tensor2做cross_entropy\n",
    "cross_entropy_2 = F.cross_entropy(tensor1.t(), torch.arange(len(tensor1)))\n",
    "\n",
    "cross_entropy_1, cross_entropy_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "lables = F.one_hot(torch.arange(start=0, end=64, dtype=torch.int64), num_classes=64).float()\n",
    "print(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7694, -2.4397,  0.8835,  0.3638, -0.3139],\n",
      "        [-0.1560,  0.5092,  0.5933,  2.5126,  0.4168],\n",
      "        [ 0.0857,  0.2075, -0.1624,  1.7552, -0.1749],\n",
      "        [-1.5560, -0.9329,  2.4862, -0.3974, -1.3995],\n",
      "        [ 1.4267, -0.3341, -0.8650,  0.1747,  0.9371]])\n",
      "tensor([[-0.7694, -2.4397,  0.8835,  0.3638, -0.3139],\n",
      "        [-0.1560,  0.5092,  0.5933,  2.5126,  0.4168],\n",
      "        [ 0.0857,  0.2075, -0.1624,  1.7552, -0.1749],\n",
      "        [-1.5560, -0.9329,  2.4862, -0.3974, -1.3995],\n",
      "        [ 1.4267, -0.3341, -0.8650,  0.1747,  0.9371]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(5, 5)\n",
    "print(tensor1.t())\n",
    "print(torch.transpose(tensor1, 0, 1))\n",
    "print(torch.equal(tensor1.t(), torch.transpose(tensor1, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 64])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(64,2048).t().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ecg_feature', 'cmr_feature')\n",
      "('ecg_feature', 'tar_feature')\n",
      "('cmr_feature', 'tar_feature')\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "output_dict = {'ecg_feature': 1, 'cmr_feature': 2, 'tar_feature': 3}\n",
    "all_combinations = combinations(output_dict.keys(), 2)\n",
    "for key_combination in all_combinations:\n",
    "    print(key_combination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3428, 0.3307],\n",
      "        [0.1235, 0.9866]])\n",
      "tensor([[0.3428, 0.3307],\n",
      "        [0.1235, 0.9866]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2,2)\n",
    "print(x[:,0,:])\n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_ecg_data', 'val_tar_data', 'val_snp_data', 'val_cha_data', 'val_I21_data', 'val_I42_data', 'val_I48_data', 'val_I50_data', 'val_cmr_data', 'val_eid'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "dict_data = torch.load('/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/val_data_dict_v6.pt')\n",
    "print(dict_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5504371)\n",
      "torch.Size([12, 5000])\n",
      "torch.Size([50, 80, 80])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "for i,eid in enumerate(dict_data['val_eid']):\n",
    "    if i == 10:\n",
    "        print(eid)\n",
    "        ecg = dict_data['val_ecg_data'][i]\n",
    "        cmr = dict_data['val_cmr_data'][i]\n",
    "        print(ecg.shape)\n",
    "        plt.plot(ecg[0])\n",
    "        print(cmr.shape) \n",
    "        nib.save(nib.Nifti1Image(np.array(cmr), np.eye(4)), f'/home/dingzhengyao/Work/ECG_CMR/ECG_CMR_TAR/Project_version1/test_code/test_generate/val_cmr_{eid}.nii.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 80, 80)\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import get_img\n",
    "import nibabel\n",
    "real_cmr = get_img('5504371_20209_2_0/sa.nii.gz')\n",
    "print(real_cmr.shape)\n",
    "nib.save(nib.Nifti1Image(np.array(real_cmr), np.eye(4)), f'/home/dingzhengyao/Work/ECG_CMR/ECG_CMR_TAR/Project_version1/test_code/test_generate/real_cmr.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import matplotlib.pyplot as plt\n",
    "def get_ecg( ecg_path):\n",
    "    ecg_file = open(ecg_path).read()\n",
    "    bs = None\n",
    "    try:\n",
    "        bs = BeautifulSoup(ecg_file, features=\"lxml\")\n",
    "    except:\n",
    "        pass\n",
    "    ecg_waveform_length = 5000\n",
    "    if ecg_waveform_length == 600:\n",
    "        waveform = bs.body.cardiologyxml.mediansamples\n",
    "    else:\n",
    "        waveform = bs.body.cardiologyxml.stripdata\n",
    "    # print(waveform)\n",
    "    # print(type(waveform))\n",
    "    data_numpy = None\n",
    "    bs_measurement = bs.body.cardiologyxml.restingecgmeasurements\n",
    "    heartbeat = int(bs_measurement.find_all(\"VentricularRate\".lower())[0].string)\n",
    "\n",
    "    for each_wave in waveform.find_all(\"waveformdata\"):\n",
    "        each_data = each_wave.string.strip().split(\",\")\n",
    "        each_data = [s.replace('\\n\\t\\t', '') for s in each_data]\n",
    "        each_data = np.array(each_data, dtype=np.float32)\n",
    "        # plt.plot(each_data)\n",
    "        seasonal_decompose_result = seasonal_decompose(each_data, model=\"additive\",\n",
    "                                                        period=int(ecg_waveform_length*6/heartbeat))\n",
    "        trend = seasonal_decompose_result.trend\n",
    "        start, end = 0, ecg_waveform_length - 1\n",
    "        sflag, eflag = False, False\n",
    "        for i in range(ecg_waveform_length):\n",
    "            if np.isnan(trend[i]):\n",
    "                start += 1\n",
    "            else:\n",
    "                sflag = True\n",
    "            if np.isnan(trend[ecg_waveform_length-1-i]):\n",
    "                end -= 1\n",
    "            else:\n",
    "                eflag = True\n",
    "            if sflag and eflag:\n",
    "                break\n",
    "        trend[:start] = trend[start]\n",
    "        trend[end:] = trend[end]\n",
    "        # trend[np.isnan(trend)] = 0.0\n",
    "        result = np.array(seasonal_decompose_result.observed - trend)\n",
    "        # plt.plot(result)\n",
    "        # plt.show()\n",
    "        # exit()\n",
    "        if data_numpy is None:\n",
    "            data_numpy = result\n",
    "        else:\n",
    "            data_numpy = np.vstack((data_numpy, result))\n",
    "\n",
    "    return data_numpy\n",
    "real_data = get_ecg('/mnt/data/ukb_heartmri/ukb_20205/5504371_20205_2_0.xml')\n",
    "real_data = torch.from_numpy(real_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1202e+01, -2.1202e+01, -2.1202e+01,  ...,  9.5064e+00,\n",
      "          1.0506e+01,  8.5064e+00],\n",
      "        [-1.3829e+01, -1.3829e+01, -7.8294e+00,  ...,  4.2670e+01,\n",
      "          4.1670e+01,  4.0670e+01],\n",
      "        [ 7.3725e+00,  7.3725e+00,  1.3372e+01,  ...,  3.3163e+01,\n",
      "          3.1163e+01,  3.2163e+01],\n",
      "        ...,\n",
      "        [-2.0330e+00, -3.3028e-02,  4.9670e+00,  ...,  8.0760e+01,\n",
      "          7.7760e+01,  7.6760e+01],\n",
      "        [-1.2178e+01, -1.2178e+01, -7.1780e+00,  ...,  4.8539e+01,\n",
      "          4.5539e+01,  4.1539e+01],\n",
      "        [-1.5310e+01, -1.5310e+01, -1.2310e+01,  ...,  3.0167e+01,\n",
      "          2.7167e+01,  2.5167e+01]])\n",
      "tensor([[-2.1202e+01, -2.1202e+01, -2.1202e+01,  ...,  9.5064e+00,\n",
      "          1.0506e+01,  8.5064e+00],\n",
      "        [-1.3829e+01, -1.3829e+01, -7.8294e+00,  ...,  4.2670e+01,\n",
      "          4.1670e+01,  4.0670e+01],\n",
      "        [ 7.3725e+00,  7.3725e+00,  1.3372e+01,  ...,  3.3163e+01,\n",
      "          3.1163e+01,  3.2163e+01],\n",
      "        ...,\n",
      "        [-2.0330e+00, -3.3028e-02,  4.9670e+00,  ...,  8.0760e+01,\n",
      "          7.7760e+01,  7.6760e+01],\n",
      "        [-1.2178e+01, -1.2178e+01, -7.1780e+00,  ...,  4.8539e+01,\n",
      "          4.5539e+01,  4.1539e+01],\n",
      "        [-1.5310e+01, -1.5310e+01, -1.2310e+01,  ...,  3.0167e+01,\n",
      "          2.7167e+01,  2.5167e+01]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(real_data)\n",
    "print(ecg)\n",
    "print(torch.equal(real_data, ecg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 80, 80])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Anaconda3/envs/pl/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "data = torch.randn(50,80,80)\n",
    "transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(size=(80,80),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "])\n",
    "data = transforms(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487 -1.06904497 -0.91202919 -0.75220493]\n",
      " [ 0.          0.         -0.26726124 -0.48001536 -0.66102858]\n",
      " [ 0.          1.22474487  1.33630621  1.39204455  1.41323351]]\n",
      "[[-1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356]]\n",
      "[[1. 1.]\n",
      " [2. 3.]\n",
      " [3. 5.]\n",
      " [4. 7.]\n",
      " [5. 9.]]\n",
      "[[ 5.82842712 10.65685425]\n",
      " [ 7.24264069 13.48528137]\n",
      " [12.89949494 16.3137085 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y_train = [[1,2,3,4,5],[1,3,5,7,9],[1,4,9,20,100]]\n",
    "# 假设 y_train 是一个包含70个变量的矩阵\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对训练数据进行标准化\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "\n",
    "print(y_train_scaled)\n",
    "scaler = StandardScaler()\n",
    "y_test = [[1,1],[2,3],[3,5],[4,7],[5,9]]\n",
    "y_test_scaled = scaler.fit_transform(y_test)\n",
    "print(y_test_scaled)\n",
    "print(scaler.inverse_transform(y_test_scaled))\n",
    "print(scaler.inverse_transform([[2,2],[3,3],[7,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['ECG_encoder.cls_token', 'ECG_encoder.pos_embed', 'ECG_encoder.patch_embed.proj.weight', 'ECG_encoder.patch_embed.proj.bias', 'ECG_encoder.blocks.0.norm1.weight', 'ECG_encoder.blocks.0.norm1.bias', 'ECG_encoder.blocks.0.attn.qkv.weight', 'ECG_encoder.blocks.0.attn.qkv.bias', 'ECG_encoder.blocks.0.attn.proj.weight', 'ECG_encoder.blocks.0.attn.proj.bias', 'ECG_encoder.blocks.0.norm2.weight', 'ECG_encoder.blocks.0.norm2.bias', 'ECG_encoder.blocks.0.mlp.fc1.weight', 'ECG_encoder.blocks.0.mlp.fc1.bias', 'ECG_encoder.blocks.0.mlp.fc2.weight', 'ECG_encoder.blocks.0.mlp.fc2.bias', 'ECG_encoder.blocks.1.norm1.weight', 'ECG_encoder.blocks.1.norm1.bias', 'ECG_encoder.blocks.1.attn.qkv.weight', 'ECG_encoder.blocks.1.attn.qkv.bias', 'ECG_encoder.blocks.1.attn.proj.weight', 'ECG_encoder.blocks.1.attn.proj.bias', 'ECG_encoder.blocks.1.norm2.weight', 'ECG_encoder.blocks.1.norm2.bias', 'ECG_encoder.blocks.1.mlp.fc1.weight', 'ECG_encoder.blocks.1.mlp.fc1.bias', 'ECG_encoder.blocks.1.mlp.fc2.weight', 'ECG_encoder.blocks.1.mlp.fc2.bias', 'ECG_encoder.blocks.2.norm1.weight', 'ECG_encoder.blocks.2.norm1.bias', 'ECG_encoder.blocks.2.attn.qkv.weight', 'ECG_encoder.blocks.2.attn.qkv.bias', 'ECG_encoder.blocks.2.attn.proj.weight', 'ECG_encoder.blocks.2.attn.proj.bias', 'ECG_encoder.blocks.2.norm2.weight', 'ECG_encoder.blocks.2.norm2.bias', 'ECG_encoder.blocks.2.mlp.fc1.weight', 'ECG_encoder.blocks.2.mlp.fc1.bias', 'ECG_encoder.blocks.2.mlp.fc2.weight', 'ECG_encoder.blocks.2.mlp.fc2.bias', 'ECG_encoder.blocks.3.norm1.weight', 'ECG_encoder.blocks.3.norm1.bias', 'ECG_encoder.blocks.3.attn.qkv.weight', 'ECG_encoder.blocks.3.attn.qkv.bias', 'ECG_encoder.blocks.3.attn.proj.weight', 'ECG_encoder.blocks.3.attn.proj.bias', 'ECG_encoder.blocks.3.norm2.weight', 'ECG_encoder.blocks.3.norm2.bias', 'ECG_encoder.blocks.3.mlp.fc1.weight', 'ECG_encoder.blocks.3.mlp.fc1.bias', 'ECG_encoder.blocks.3.mlp.fc2.weight', 'ECG_encoder.blocks.3.mlp.fc2.bias', 'ECG_encoder.blocks.4.norm1.weight', 'ECG_encoder.blocks.4.norm1.bias', 'ECG_encoder.blocks.4.attn.qkv.weight', 'ECG_encoder.blocks.4.attn.qkv.bias', 'ECG_encoder.blocks.4.attn.proj.weight', 'ECG_encoder.blocks.4.attn.proj.bias', 'ECG_encoder.blocks.4.norm2.weight', 'ECG_encoder.blocks.4.norm2.bias', 'ECG_encoder.blocks.4.mlp.fc1.weight', 'ECG_encoder.blocks.4.mlp.fc1.bias', 'ECG_encoder.blocks.4.mlp.fc2.weight', 'ECG_encoder.blocks.4.mlp.fc2.bias', 'ECG_encoder.blocks.5.norm1.weight', 'ECG_encoder.blocks.5.norm1.bias', 'ECG_encoder.blocks.5.attn.qkv.weight', 'ECG_encoder.blocks.5.attn.qkv.bias', 'ECG_encoder.blocks.5.attn.proj.weight', 'ECG_encoder.blocks.5.attn.proj.bias', 'ECG_encoder.blocks.5.norm2.weight', 'ECG_encoder.blocks.5.norm2.bias', 'ECG_encoder.blocks.5.mlp.fc1.weight', 'ECG_encoder.blocks.5.mlp.fc1.bias', 'ECG_encoder.blocks.5.mlp.fc2.weight', 'ECG_encoder.blocks.5.mlp.fc2.bias', 'ECG_encoder.blocks.6.norm1.weight', 'ECG_encoder.blocks.6.norm1.bias', 'ECG_encoder.blocks.6.attn.qkv.weight', 'ECG_encoder.blocks.6.attn.qkv.bias', 'ECG_encoder.blocks.6.attn.proj.weight', 'ECG_encoder.blocks.6.attn.proj.bias', 'ECG_encoder.blocks.6.norm2.weight', 'ECG_encoder.blocks.6.norm2.bias', 'ECG_encoder.blocks.6.mlp.fc1.weight', 'ECG_encoder.blocks.6.mlp.fc1.bias', 'ECG_encoder.blocks.6.mlp.fc2.weight', 'ECG_encoder.blocks.6.mlp.fc2.bias', 'ECG_encoder.blocks.7.norm1.weight', 'ECG_encoder.blocks.7.norm1.bias', 'ECG_encoder.blocks.7.attn.qkv.weight', 'ECG_encoder.blocks.7.attn.qkv.bias', 'ECG_encoder.blocks.7.attn.proj.weight', 'ECG_encoder.blocks.7.attn.proj.bias', 'ECG_encoder.blocks.7.norm2.weight', 'ECG_encoder.blocks.7.norm2.bias', 'ECG_encoder.blocks.7.mlp.fc1.weight', 'ECG_encoder.blocks.7.mlp.fc1.bias', 'ECG_encoder.blocks.7.mlp.fc2.weight', 'ECG_encoder.blocks.7.mlp.fc2.bias', 'ECG_encoder.blocks.8.norm1.weight', 'ECG_encoder.blocks.8.norm1.bias', 'ECG_encoder.blocks.8.attn.qkv.weight', 'ECG_encoder.blocks.8.attn.qkv.bias', 'ECG_encoder.blocks.8.attn.proj.weight', 'ECG_encoder.blocks.8.attn.proj.bias', 'ECG_encoder.blocks.8.norm2.weight', 'ECG_encoder.blocks.8.norm2.bias', 'ECG_encoder.blocks.8.mlp.fc1.weight', 'ECG_encoder.blocks.8.mlp.fc1.bias', 'ECG_encoder.blocks.8.mlp.fc2.weight', 'ECG_encoder.blocks.8.mlp.fc2.bias', 'ECG_encoder.blocks.9.norm1.weight', 'ECG_encoder.blocks.9.norm1.bias', 'ECG_encoder.blocks.9.attn.qkv.weight', 'ECG_encoder.blocks.9.attn.qkv.bias', 'ECG_encoder.blocks.9.attn.proj.weight', 'ECG_encoder.blocks.9.attn.proj.bias', 'ECG_encoder.blocks.9.norm2.weight', 'ECG_encoder.blocks.9.norm2.bias', 'ECG_encoder.blocks.9.mlp.fc1.weight', 'ECG_encoder.blocks.9.mlp.fc1.bias', 'ECG_encoder.blocks.9.mlp.fc2.weight', 'ECG_encoder.blocks.9.mlp.fc2.bias', 'ECG_encoder.blocks.10.norm1.weight', 'ECG_encoder.blocks.10.norm1.bias', 'ECG_encoder.blocks.10.attn.qkv.weight', 'ECG_encoder.blocks.10.attn.qkv.bias', 'ECG_encoder.blocks.10.attn.proj.weight', 'ECG_encoder.blocks.10.attn.proj.bias', 'ECG_encoder.blocks.10.norm2.weight', 'ECG_encoder.blocks.10.norm2.bias', 'ECG_encoder.blocks.10.mlp.fc1.weight', 'ECG_encoder.blocks.10.mlp.fc1.bias', 'ECG_encoder.blocks.10.mlp.fc2.weight', 'ECG_encoder.blocks.10.mlp.fc2.bias', 'ECG_encoder.blocks.11.norm1.weight', 'ECG_encoder.blocks.11.norm1.bias', 'ECG_encoder.blocks.11.attn.qkv.weight', 'ECG_encoder.blocks.11.attn.qkv.bias', 'ECG_encoder.blocks.11.attn.proj.weight', 'ECG_encoder.blocks.11.attn.proj.bias', 'ECG_encoder.blocks.11.norm2.weight', 'ECG_encoder.blocks.11.norm2.bias', 'ECG_encoder.blocks.11.mlp.fc1.weight', 'ECG_encoder.blocks.11.mlp.fc1.bias', 'ECG_encoder.blocks.11.mlp.fc2.weight', 'ECG_encoder.blocks.11.mlp.fc2.bias', 'ECG_encoder.norm.weight', 'ECG_encoder.norm.bias', 'ECG_encoder.head.weight', 'ECG_encoder.head.bias', 'CMR_encoder.cls_token', 'CMR_encoder.pos_embed', 'CMR_encoder.patch_embed.proj.weight', 'CMR_encoder.patch_embed.proj.bias', 'CMR_encoder.blocks.0.norm1.weight', 'CMR_encoder.blocks.0.norm1.bias', 'CMR_encoder.blocks.0.attn.qkv.weight', 'CMR_encoder.blocks.0.attn.qkv.bias', 'CMR_encoder.blocks.0.attn.proj.weight', 'CMR_encoder.blocks.0.attn.proj.bias', 'CMR_encoder.blocks.0.norm2.weight', 'CMR_encoder.blocks.0.norm2.bias', 'CMR_encoder.blocks.0.mlp.fc1.weight', 'CMR_encoder.blocks.0.mlp.fc1.bias', 'CMR_encoder.blocks.0.mlp.fc2.weight', 'CMR_encoder.blocks.0.mlp.fc2.bias', 'CMR_encoder.blocks.1.norm1.weight', 'CMR_encoder.blocks.1.norm1.bias', 'CMR_encoder.blocks.1.attn.qkv.weight', 'CMR_encoder.blocks.1.attn.qkv.bias', 'CMR_encoder.blocks.1.attn.proj.weight', 'CMR_encoder.blocks.1.attn.proj.bias', 'CMR_encoder.blocks.1.norm2.weight', 'CMR_encoder.blocks.1.norm2.bias', 'CMR_encoder.blocks.1.mlp.fc1.weight', 'CMR_encoder.blocks.1.mlp.fc1.bias', 'CMR_encoder.blocks.1.mlp.fc2.weight', 'CMR_encoder.blocks.1.mlp.fc2.bias', 'CMR_encoder.blocks.2.norm1.weight', 'CMR_encoder.blocks.2.norm1.bias', 'CMR_encoder.blocks.2.attn.qkv.weight', 'CMR_encoder.blocks.2.attn.qkv.bias', 'CMR_encoder.blocks.2.attn.proj.weight', 'CMR_encoder.blocks.2.attn.proj.bias', 'CMR_encoder.blocks.2.norm2.weight', 'CMR_encoder.blocks.2.norm2.bias', 'CMR_encoder.blocks.2.mlp.fc1.weight', 'CMR_encoder.blocks.2.mlp.fc1.bias', 'CMR_encoder.blocks.2.mlp.fc2.weight', 'CMR_encoder.blocks.2.mlp.fc2.bias', 'CMR_encoder.blocks.3.norm1.weight', 'CMR_encoder.blocks.3.norm1.bias', 'CMR_encoder.blocks.3.attn.qkv.weight', 'CMR_encoder.blocks.3.attn.qkv.bias', 'CMR_encoder.blocks.3.attn.proj.weight', 'CMR_encoder.blocks.3.attn.proj.bias', 'CMR_encoder.blocks.3.norm2.weight', 'CMR_encoder.blocks.3.norm2.bias', 'CMR_encoder.blocks.3.mlp.fc1.weight', 'CMR_encoder.blocks.3.mlp.fc1.bias', 'CMR_encoder.blocks.3.mlp.fc2.weight', 'CMR_encoder.blocks.3.mlp.fc2.bias', 'CMR_encoder.blocks.4.norm1.weight', 'CMR_encoder.blocks.4.norm1.bias', 'CMR_encoder.blocks.4.attn.qkv.weight', 'CMR_encoder.blocks.4.attn.qkv.bias', 'CMR_encoder.blocks.4.attn.proj.weight', 'CMR_encoder.blocks.4.attn.proj.bias', 'CMR_encoder.blocks.4.norm2.weight', 'CMR_encoder.blocks.4.norm2.bias', 'CMR_encoder.blocks.4.mlp.fc1.weight', 'CMR_encoder.blocks.4.mlp.fc1.bias', 'CMR_encoder.blocks.4.mlp.fc2.weight', 'CMR_encoder.blocks.4.mlp.fc2.bias', 'CMR_encoder.blocks.5.norm1.weight', 'CMR_encoder.blocks.5.norm1.bias', 'CMR_encoder.blocks.5.attn.qkv.weight', 'CMR_encoder.blocks.5.attn.qkv.bias', 'CMR_encoder.blocks.5.attn.proj.weight', 'CMR_encoder.blocks.5.attn.proj.bias', 'CMR_encoder.blocks.5.norm2.weight', 'CMR_encoder.blocks.5.norm2.bias', 'CMR_encoder.blocks.5.mlp.fc1.weight', 'CMR_encoder.blocks.5.mlp.fc1.bias', 'CMR_encoder.blocks.5.mlp.fc2.weight', 'CMR_encoder.blocks.5.mlp.fc2.bias', 'CMR_encoder.blocks.6.norm1.weight', 'CMR_encoder.blocks.6.norm1.bias', 'CMR_encoder.blocks.6.attn.qkv.weight', 'CMR_encoder.blocks.6.attn.qkv.bias', 'CMR_encoder.blocks.6.attn.proj.weight', 'CMR_encoder.blocks.6.attn.proj.bias', 'CMR_encoder.blocks.6.norm2.weight', 'CMR_encoder.blocks.6.norm2.bias', 'CMR_encoder.blocks.6.mlp.fc1.weight', 'CMR_encoder.blocks.6.mlp.fc1.bias', 'CMR_encoder.blocks.6.mlp.fc2.weight', 'CMR_encoder.blocks.6.mlp.fc2.bias', 'CMR_encoder.blocks.7.norm1.weight', 'CMR_encoder.blocks.7.norm1.bias', 'CMR_encoder.blocks.7.attn.qkv.weight', 'CMR_encoder.blocks.7.attn.qkv.bias', 'CMR_encoder.blocks.7.attn.proj.weight', 'CMR_encoder.blocks.7.attn.proj.bias', 'CMR_encoder.blocks.7.norm2.weight', 'CMR_encoder.blocks.7.norm2.bias', 'CMR_encoder.blocks.7.mlp.fc1.weight', 'CMR_encoder.blocks.7.mlp.fc1.bias', 'CMR_encoder.blocks.7.mlp.fc2.weight', 'CMR_encoder.blocks.7.mlp.fc2.bias', 'CMR_encoder.blocks.8.norm1.weight', 'CMR_encoder.blocks.8.norm1.bias', 'CMR_encoder.blocks.8.attn.qkv.weight', 'CMR_encoder.blocks.8.attn.qkv.bias', 'CMR_encoder.blocks.8.attn.proj.weight', 'CMR_encoder.blocks.8.attn.proj.bias', 'CMR_encoder.blocks.8.norm2.weight', 'CMR_encoder.blocks.8.norm2.bias', 'CMR_encoder.blocks.8.mlp.fc1.weight', 'CMR_encoder.blocks.8.mlp.fc1.bias', 'CMR_encoder.blocks.8.mlp.fc2.weight', 'CMR_encoder.blocks.8.mlp.fc2.bias', 'CMR_encoder.blocks.9.norm1.weight', 'CMR_encoder.blocks.9.norm1.bias', 'CMR_encoder.blocks.9.attn.qkv.weight', 'CMR_encoder.blocks.9.attn.qkv.bias', 'CMR_encoder.blocks.9.attn.proj.weight', 'CMR_encoder.blocks.9.attn.proj.bias', 'CMR_encoder.blocks.9.norm2.weight', 'CMR_encoder.blocks.9.norm2.bias', 'CMR_encoder.blocks.9.mlp.fc1.weight', 'CMR_encoder.blocks.9.mlp.fc1.bias', 'CMR_encoder.blocks.9.mlp.fc2.weight', 'CMR_encoder.blocks.9.mlp.fc2.bias', 'CMR_encoder.blocks.10.norm1.weight', 'CMR_encoder.blocks.10.norm1.bias', 'CMR_encoder.blocks.10.attn.qkv.weight', 'CMR_encoder.blocks.10.attn.qkv.bias', 'CMR_encoder.blocks.10.attn.proj.weight', 'CMR_encoder.blocks.10.attn.proj.bias', 'CMR_encoder.blocks.10.norm2.weight', 'CMR_encoder.blocks.10.norm2.bias', 'CMR_encoder.blocks.10.mlp.fc1.weight', 'CMR_encoder.blocks.10.mlp.fc1.bias', 'CMR_encoder.blocks.10.mlp.fc2.weight', 'CMR_encoder.blocks.10.mlp.fc2.bias', 'CMR_encoder.blocks.11.norm1.weight', 'CMR_encoder.blocks.11.norm1.bias', 'CMR_encoder.blocks.11.attn.qkv.weight', 'CMR_encoder.blocks.11.attn.qkv.bias', 'CMR_encoder.blocks.11.attn.proj.weight', 'CMR_encoder.blocks.11.attn.proj.bias', 'CMR_encoder.blocks.11.norm2.weight', 'CMR_encoder.blocks.11.norm2.bias', 'CMR_encoder.blocks.11.mlp.fc1.weight', 'CMR_encoder.blocks.11.mlp.fc1.bias', 'CMR_encoder.blocks.11.mlp.fc2.weight', 'CMR_encoder.blocks.11.mlp.fc2.bias', 'CMR_encoder.norm.weight', 'CMR_encoder.norm.bias', 'CMR_encoder.head.weight', 'CMR_encoder.head.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ecg_checkpoint = torch.load('/mnt/data/dingzhengyao/work/checkpoint/preject_version1/output_dir/7005/checkpoint-67-loss-2.76.pth', map_location='cpu')\n",
    "ecg_checkpoint_model = ecg_checkpoint['model']\n",
    "print(ecg_checkpoint_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_encoder_keys = {k: v for k, v in ecg_checkpoint_model.items() if k.startswith('ECG_encoder')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.ECGEncoder as ECGEncoder\n",
    "model = ECGEncoder.__dict__[args.ecg_model](\n",
    "                img_size=(12, 5000),\n",
    "                patch_size=(1, 500),\n",
    "                in_chans=1,\n",
    "                num_classes=82,\n",
    "                drop_rate=0.1,\n",
    "                args=args,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ECG_encoder.cls_token', 'ECG_encoder.pos_embed', 'ECG_encoder.patch_embed.proj.weight', 'ECG_encoder.patch_embed.proj.bias', 'ECG_encoder.blocks.0.norm1.weight', 'ECG_encoder.blocks.0.norm1.bias', 'ECG_encoder.blocks.0.attn.qkv.weight', 'ECG_encoder.blocks.0.attn.qkv.bias', 'ECG_encoder.blocks.0.attn.proj.weight', 'ECG_encoder.blocks.0.attn.proj.bias', 'ECG_encoder.blocks.0.norm2.weight', 'ECG_encoder.blocks.0.norm2.bias', 'ECG_encoder.blocks.0.mlp.fc1.weight', 'ECG_encoder.blocks.0.mlp.fc1.bias', 'ECG_encoder.blocks.0.mlp.fc2.weight', 'ECG_encoder.blocks.0.mlp.fc2.bias', 'ECG_encoder.blocks.1.norm1.weight', 'ECG_encoder.blocks.1.norm1.bias', 'ECG_encoder.blocks.1.attn.qkv.weight', 'ECG_encoder.blocks.1.attn.qkv.bias', 'ECG_encoder.blocks.1.attn.proj.weight', 'ECG_encoder.blocks.1.attn.proj.bias', 'ECG_encoder.blocks.1.norm2.weight', 'ECG_encoder.blocks.1.norm2.bias', 'ECG_encoder.blocks.1.mlp.fc1.weight', 'ECG_encoder.blocks.1.mlp.fc1.bias', 'ECG_encoder.blocks.1.mlp.fc2.weight', 'ECG_encoder.blocks.1.mlp.fc2.bias', 'ECG_encoder.blocks.2.norm1.weight', 'ECG_encoder.blocks.2.norm1.bias', 'ECG_encoder.blocks.2.attn.qkv.weight', 'ECG_encoder.blocks.2.attn.qkv.bias', 'ECG_encoder.blocks.2.attn.proj.weight', 'ECG_encoder.blocks.2.attn.proj.bias', 'ECG_encoder.blocks.2.norm2.weight', 'ECG_encoder.blocks.2.norm2.bias', 'ECG_encoder.blocks.2.mlp.fc1.weight', 'ECG_encoder.blocks.2.mlp.fc1.bias', 'ECG_encoder.blocks.2.mlp.fc2.weight', 'ECG_encoder.blocks.2.mlp.fc2.bias', 'ECG_encoder.blocks.3.norm1.weight', 'ECG_encoder.blocks.3.norm1.bias', 'ECG_encoder.blocks.3.attn.qkv.weight', 'ECG_encoder.blocks.3.attn.qkv.bias', 'ECG_encoder.blocks.3.attn.proj.weight', 'ECG_encoder.blocks.3.attn.proj.bias', 'ECG_encoder.blocks.3.norm2.weight', 'ECG_encoder.blocks.3.norm2.bias', 'ECG_encoder.blocks.3.mlp.fc1.weight', 'ECG_encoder.blocks.3.mlp.fc1.bias', 'ECG_encoder.blocks.3.mlp.fc2.weight', 'ECG_encoder.blocks.3.mlp.fc2.bias', 'ECG_encoder.blocks.4.norm1.weight', 'ECG_encoder.blocks.4.norm1.bias', 'ECG_encoder.blocks.4.attn.qkv.weight', 'ECG_encoder.blocks.4.attn.qkv.bias', 'ECG_encoder.blocks.4.attn.proj.weight', 'ECG_encoder.blocks.4.attn.proj.bias', 'ECG_encoder.blocks.4.norm2.weight', 'ECG_encoder.blocks.4.norm2.bias', 'ECG_encoder.blocks.4.mlp.fc1.weight', 'ECG_encoder.blocks.4.mlp.fc1.bias', 'ECG_encoder.blocks.4.mlp.fc2.weight', 'ECG_encoder.blocks.4.mlp.fc2.bias', 'ECG_encoder.blocks.5.norm1.weight', 'ECG_encoder.blocks.5.norm1.bias', 'ECG_encoder.blocks.5.attn.qkv.weight', 'ECG_encoder.blocks.5.attn.qkv.bias', 'ECG_encoder.blocks.5.attn.proj.weight', 'ECG_encoder.blocks.5.attn.proj.bias', 'ECG_encoder.blocks.5.norm2.weight', 'ECG_encoder.blocks.5.norm2.bias', 'ECG_encoder.blocks.5.mlp.fc1.weight', 'ECG_encoder.blocks.5.mlp.fc1.bias', 'ECG_encoder.blocks.5.mlp.fc2.weight', 'ECG_encoder.blocks.5.mlp.fc2.bias', 'ECG_encoder.blocks.6.norm1.weight', 'ECG_encoder.blocks.6.norm1.bias', 'ECG_encoder.blocks.6.attn.qkv.weight', 'ECG_encoder.blocks.6.attn.qkv.bias', 'ECG_encoder.blocks.6.attn.proj.weight', 'ECG_encoder.blocks.6.attn.proj.bias', 'ECG_encoder.blocks.6.norm2.weight', 'ECG_encoder.blocks.6.norm2.bias', 'ECG_encoder.blocks.6.mlp.fc1.weight', 'ECG_encoder.blocks.6.mlp.fc1.bias', 'ECG_encoder.blocks.6.mlp.fc2.weight', 'ECG_encoder.blocks.6.mlp.fc2.bias', 'ECG_encoder.blocks.7.norm1.weight', 'ECG_encoder.blocks.7.norm1.bias', 'ECG_encoder.blocks.7.attn.qkv.weight', 'ECG_encoder.blocks.7.attn.qkv.bias', 'ECG_encoder.blocks.7.attn.proj.weight', 'ECG_encoder.blocks.7.attn.proj.bias', 'ECG_encoder.blocks.7.norm2.weight', 'ECG_encoder.blocks.7.norm2.bias', 'ECG_encoder.blocks.7.mlp.fc1.weight', 'ECG_encoder.blocks.7.mlp.fc1.bias', 'ECG_encoder.blocks.7.mlp.fc2.weight', 'ECG_encoder.blocks.7.mlp.fc2.bias', 'ECG_encoder.blocks.8.norm1.weight', 'ECG_encoder.blocks.8.norm1.bias', 'ECG_encoder.blocks.8.attn.qkv.weight', 'ECG_encoder.blocks.8.attn.qkv.bias', 'ECG_encoder.blocks.8.attn.proj.weight', 'ECG_encoder.blocks.8.attn.proj.bias', 'ECG_encoder.blocks.8.norm2.weight', 'ECG_encoder.blocks.8.norm2.bias', 'ECG_encoder.blocks.8.mlp.fc1.weight', 'ECG_encoder.blocks.8.mlp.fc1.bias', 'ECG_encoder.blocks.8.mlp.fc2.weight', 'ECG_encoder.blocks.8.mlp.fc2.bias', 'ECG_encoder.blocks.9.norm1.weight', 'ECG_encoder.blocks.9.norm1.bias', 'ECG_encoder.blocks.9.attn.qkv.weight', 'ECG_encoder.blocks.9.attn.qkv.bias', 'ECG_encoder.blocks.9.attn.proj.weight', 'ECG_encoder.blocks.9.attn.proj.bias', 'ECG_encoder.blocks.9.norm2.weight', 'ECG_encoder.blocks.9.norm2.bias', 'ECG_encoder.blocks.9.mlp.fc1.weight', 'ECG_encoder.blocks.9.mlp.fc1.bias', 'ECG_encoder.blocks.9.mlp.fc2.weight', 'ECG_encoder.blocks.9.mlp.fc2.bias', 'ECG_encoder.blocks.10.norm1.weight', 'ECG_encoder.blocks.10.norm1.bias', 'ECG_encoder.blocks.10.attn.qkv.weight', 'ECG_encoder.blocks.10.attn.qkv.bias', 'ECG_encoder.blocks.10.attn.proj.weight', 'ECG_encoder.blocks.10.attn.proj.bias', 'ECG_encoder.blocks.10.norm2.weight', 'ECG_encoder.blocks.10.norm2.bias', 'ECG_encoder.blocks.10.mlp.fc1.weight', 'ECG_encoder.blocks.10.mlp.fc1.bias', 'ECG_encoder.blocks.10.mlp.fc2.weight', 'ECG_encoder.blocks.10.mlp.fc2.bias', 'ECG_encoder.blocks.11.norm1.weight', 'ECG_encoder.blocks.11.norm1.bias', 'ECG_encoder.blocks.11.attn.qkv.weight', 'ECG_encoder.blocks.11.attn.qkv.bias', 'ECG_encoder.blocks.11.attn.proj.weight', 'ECG_encoder.blocks.11.attn.proj.bias', 'ECG_encoder.blocks.11.norm2.weight', 'ECG_encoder.blocks.11.norm2.bias', 'ECG_encoder.blocks.11.mlp.fc1.weight', 'ECG_encoder.blocks.11.mlp.fc1.bias', 'ECG_encoder.blocks.11.mlp.fc2.weight', 'ECG_encoder.blocks.11.mlp.fc2.bias', 'ECG_encoder.norm.weight', 'ECG_encoder.norm.bias', 'ECG_encoder.head.weight', 'ECG_encoder.head.bias'])\n",
      "dict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias'])\n"
     ]
    }
   ],
   "source": [
    "new_keys = {k.replace('ECG_encoder.', ''): v for k, v in ECG_encoder_keys.items()}\n",
    "print(ECG_encoder_keys.keys())\n",
    "print(new_keys.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias'])\n"
     ]
    }
   ],
   "source": [
    "filtered_keys = {k: v for k, v in new_keys.items() if not k.startswith('head')}\n",
    "print(filtered_keys.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ECGEncoder:\n\tMissing key(s) in state_dict: \"head.weight\", \"head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/Anaconda3/envs/pl/lib/python3.9/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ECGEncoder:\n\tMissing key(s) in state_dict: \"head.weight\", \"head.bias\". "
     ]
    }
   ],
   "source": [
    "msg = model.load_state_dict(filtered_keys, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, epochs=400, accum_iter=1, downstream='regression', regression_dim=82, latent_dim=2048, snp_size=(49, 120), use_snp=True, snp_drop_out=0.0, snp_att_depth=12, snp_global_pool=False, ecg_pretrained=True, ecg_model='vit_base_patchX', ecg_pretrained_model='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/output_dir/7005/checkpoint-67-loss-2.76.pth', ecg_input_channels=1, ecg_input_electrodes=12, ecg_time_steps=5000, ecg_input_size=(12, 5000), ecg_patch_height=1, ecg_patch_width=500, ecg_patch_size=(1, 500), ecg_globle_pool=False, ecg_drop_out=0.0, norm_pix_loss=False, cmr_model='vit_base_patch8', cmr_inchannels=50, cmr_pretrained=False, img_size=80, cmr_patch_height=8, cmr_patch_width=8, cmr_drop_out=0.0, cmr_use_seg=False, cmr_use_continue=True, tar_pretrained=True, tar_number=195, tar_pretrained_path='/home/dingzhengyao/Work/ECG_CMR/tabnet/pretrain_tabnet_model_by_train_data_1.zip', tar_model='tabnet', tar_hidden_features=256, tar_drop_out=0.0, loss='clip_loss', input_size=(12, 5000), timeFlip=0.33, signFlip=0.33, weight_decay=0.05, lr=None, blr=0.0001, min_lr=0.0, warmup_epochs=40, patience=20, max_delta=0.2, data_path='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/train_data_dict_v6.pt', val_data_path='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/val_data_dict_v6.pt', test_data_path='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/test_data_dict_v6.pt', output_dir='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/finetune_output_dir', log_dir='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/finetune_log_dir', wandb=True, wandb_project='CMR_ECG_TAR_finetune', wandb_id='1001', device='cuda:3', seed=0, resume='', start_epoch=0, num_workers=8, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Tuple\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    # Basic parameters\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory '\n",
    "                             'constraints)')\n",
    "    #downstream task\n",
    "    parser.add_argument('--downstream', default='regression', type=str, help='downstream task')\n",
    "    parser.add_argument('--regression_dim',default=82,type=int,help='regression_dim')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--latent_dim', default=2048, type=int, metavar='N',\n",
    "                        help='latent_dim')\n",
    "    # SNP parameters\n",
    "    parser.add_argument('--snp_size', default=(49, 120), type=Tuple, help='ecg input size')\n",
    "    parser.add_argument('--use_snp', default=True, type=str2bool, help='use_snp')\n",
    "    parser.add_argument('--snp_drop_out', default=0.0, type=float)\n",
    "    parser.add_argument('--snp_att_depth', default=12, type=int)\n",
    "    parser.add_argument('--snp_global_pool', default=False, type=str2bool, help='snp_global_pool')\n",
    "    # ECG Model parameters\n",
    "    parser.add_argument('--ecg_pretrained', default=True, type=str2bool,help='ecg_pretrained or not')\n",
    "    parser.add_argument('--ecg_model', default='vit_base_patchX', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--ecg_pretrained_model',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/output_dir/7005/checkpoint-67-loss-2.76.pth',\n",
    "                        type=str, metavar='MODEL', help='path of pretaained model')\n",
    "    parser.add_argument('--ecg_input_channels', type=int, default=1, metavar='N',\n",
    "                        help='ecginput_channels')\n",
    "    parser.add_argument('--ecg_input_electrodes', type=int, default=12, metavar='N',\n",
    "                        help='ecg input electrodes')\n",
    "    parser.add_argument('--ecg_time_steps', type=int, default=5000, metavar='N',\n",
    "                        help='ecg input length')\n",
    "    parser.add_argument('--ecg_input_size', default=(12, 5000), type=Tuple,\n",
    "                        help='ecg input size')\n",
    "    parser.add_argument('--ecg_patch_height', type=int, default=1, metavar='N',\n",
    "                        help='ecg patch height')\n",
    "    parser.add_argument('--ecg_patch_width', type=int, default=500, metavar='N',\n",
    "                        help='ecg patch width')\n",
    "    parser.add_argument('--ecg_patch_size', default=(1, 500), type=Tuple,\n",
    "                        help='ecg patch size')\n",
    "    parser.add_argument('--ecg_globle_pool', default=False,type=str2bool, help='ecg_globle_pool')\n",
    "    parser.add_argument('--ecg_drop_out', default=0.0, type=float)\n",
    "    parser.add_argument('--norm_pix_loss', action='store_true', default=False,\n",
    "                        help='Use (per-patch) normalized pixels as targets for computing loss')\n",
    "    \n",
    "\n",
    "    # CMR Model parameters\n",
    "    parser.add_argument('--cmr_model', default='vit_base_patch8', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--cmr_inchannels', default=50, type=int, metavar='N',\n",
    "                        help='cmr_inchannels')\n",
    "    parser.add_argument('--cmr_pretrained', default=False, type=str2bool,\n",
    "                        help='cmr_pretrained or not')\n",
    "    parser.add_argument('--img_size', default=80, type=int, metavar='N', help='img_size of cmr')\n",
    "    parser.add_argument('--cmr_patch_height', type=int, default=8, metavar='N',\n",
    "                        help='cmr patch height')\n",
    "    parser.add_argument('--cmr_patch_width', type=int, default=8, metavar='N',\n",
    "                        help='cmr patch width')\n",
    "    parser.add_argument('--cmr_drop_out', default=0.0, type=float)\n",
    "    parser.add_argument('--cmr_use_seg', default=False, type=str2bool, help='whether use seg mask')\n",
    "    parser.add_argument('--cmr_use_continue', default=True, type=str2bool, help='whether use continue data')\n",
    "    \n",
    "    # TAR Model parameters\n",
    "    parser.add_argument('--tar_pretrained', default=True, type=str2bool,help='tar_pretrained or not')\n",
    "    parser.add_argument('--tar_number', default=195, type=int, metavar='N',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--tar_pretrained_path',\n",
    "                        default='/home/dingzhengyao/Work/ECG_CMR/tabnet/pretrain_tabnet_model_by_train_data_1.zip',\n",
    "                        type=str, metavar='MODEL', help='path of pretaained model')\n",
    "    parser.add_argument('--tar_model', default='tabnet', type=str, metavar='MODEL')\n",
    "    parser.add_argument('--tar_hidden_features', default=256, type=int, metavar='N')\n",
    "    parser.add_argument('--tar_drop_out', default=0.0, type=float)\n",
    "    \n",
    "\n",
    "    # LOSS parameters\n",
    "    parser.add_argument('--loss', default='clip_loss', type=str, metavar='LOSS', help='loss function')\n",
    "    \n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--input_size', type=tuple, default=(12, 5000))\n",
    "\n",
    "    parser.add_argument('--timeFlip', type=float, default=0.33)\n",
    "\n",
    "    parser.add_argument('--signFlip', type=float, default=0.33)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--blr', type=float, default=1e-4, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=40, metavar='N',\n",
    "                        help='epochs to warmup LR')\n",
    "\n",
    "    # Callback parameters\n",
    "    parser.add_argument('--patience', default=20, type=float,\n",
    "                        help='Early stopping whether val is worse than train for specified nb of epochs (default: -1, i.e. no early stopping)')\n",
    "    parser.add_argument('--max_delta', default=0.2, type=float,\n",
    "                        help='Early stopping threshold (val has to be worse than (train+delta)) (default: 0)')\n",
    "\n",
    "    # Dataset parameters\n",
    "\n",
    "    parser.add_argument('--data_path',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/train_data_dict_v6.pt',\n",
    "                        type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--val_data_path',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/val_data_dict_v6.pt',\n",
    "                        type=str,\n",
    "                        help='validation dataset path')\n",
    "    parser.add_argument('--test_data_path',\n",
    "                        default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/data/test_data_dict_v6.pt',\n",
    "                        type=str,\n",
    "                        help='test dataset path')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/finetune_output_dir',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default='/mnt/data/dingzhengyao/work/checkpoint/preject_version1/finetune_log_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--wandb', type=str2bool,  default=True)\n",
    "    parser.add_argument('--wandb_project', default='CMR_ECG_TAR_finetune',\n",
    "                        help='project where to wandb log')\n",
    "    # parser.add_argument('--wandb_dir', default='/mnt/data/dingzhengyao/work/checkpoint/ECG_CMR/wandb/1002',\n",
    "    #                     help='project where to wandb save')\n",
    "    parser.add_argument('--wandb_id', default='1001', type=str,\n",
    "                        help='id of the current run')\n",
    "    parser.add_argument('--device', default='cuda:3',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true', default=True, \n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "\n",
    "    # Distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    return parser\n",
    "\n",
    "args = get_args_parser().parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg.shape: torch.Size([24908, 12, 5000]), cmr.shape: torch.Size([24908, 50, 80, 80]), tar.shape: torch.Size([24908, 195]), snp.shape: torch.Size([24908, 49, 120]), cha.shape: (24908, 82), I21.shape: torch.Size([24908, 1]), I42.shape: torch.Size([24908, 1]), I48.shape: torch.Size([24908, 1]), I50.shape: torch.Size([24908, 1])\n",
      "ecg.shape: torch.Size([8303, 12, 5000]), cmr.shape: torch.Size([8303, 50, 80, 80]), tar.shape: torch.Size([8303, 195]), snp.shape: torch.Size([8303, 49, 120]), cha.shape: (8303, 82), I21.shape: torch.Size([8303, 1]), I42.shape: torch.Size([8303, 1]), I48.shape: torch.Size([8303, 1]), I50.shape: torch.Size([8303, 1])\n",
      "Training set size:  24908\n",
      "Validation set size:  8303\n"
     ]
    }
   ],
   "source": [
    "from data.mutimodal_dataset import mutimodal_dataset\n",
    "dataset_train = mutimodal_dataset(data_path=args.data_path, transform=True, augment=True, args=args,downstream=args.downstream)\n",
    "data_scaler = dataset_train.get_scaler()\n",
    "dataset_val = mutimodal_dataset(data_path=args.val_data_path, transform=True, augment=False, args=args,scaler=data_scaler,downstream=args.downstream)\n",
    "print(\"Training set size: \", len(dataset_train))\n",
    "print(\"Validation set size: \", len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.CMREncoder as CMREncoder\n",
    "model = CMREncoder.__dict__[args.cmr_model](\n",
    "                        in_chans=args.cmr_inchannels,\n",
    "                        img_size=args.img_size,\n",
    "                        num_classes=args.regression_dim,\n",
    "                        drop_rate=args.cmr_drop_out,\n",
    "                        args=args,\n",
    "                    )\n",
    "print(\"load pretrained ecg_model\")\n",
    "ecg_checkpoint = torch.load('/mnt/data/dingzhengyao/work/checkpoint/preject_version1/cmr_pretrain_output_dir/1001/checkpoint-25-loss-0.87.pth' ,map_location='cpu')\n",
    "\n",
    "msg = model.load_state_dict(ecg_checkpoint['model'])\n",
    "print(msg)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        shuffle=False,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8256, 82)\n",
      "(8256, 82)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "output = []\n",
    "label = []\n",
    "for i,batch in enumerate(data_loader_val):\n",
    "    ecg, cmr, tar, snp, cha = batch\n",
    "    label.append(cha.numpy())\n",
    "    cmr = cmr.float().to(args.device)\n",
    "    # print(ecg.shape)\n",
    "    out = model(cmr)\n",
    "    # print(out.shape)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    out = out.reshape(-1, out.shape[-1])  # reshape the output\n",
    "    output.append(out)\n",
    "    \n",
    "output = np.concatenate(output, axis=0)\n",
    "label = np.concatenate(label, axis=0)\n",
    "print(output.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7556363799678247\n",
      "0.7434196465604523\n",
      "0.6654561741092595\n",
      "0.45834652426372724\n",
      "0.6080066120815275\n",
      "0.7654843563060951\n",
      "0.7607441662020084\n",
      "0.7530428801474182\n",
      "0.686382604253763\n",
      "0.6208572043444904\n",
      "0.4636053443712877\n",
      "0.4575421772834625\n",
      "0.5004990961543397\n",
      "0.39330633029165196\n",
      "0.5894623279120734\n",
      "0.5929164778343363\n",
      "0.46792190407616996\n",
      "0.35575174903878276\n",
      "0.485847801373335\n",
      "0.4948569062222254\n",
      "0.38725130617067727\n",
      "0.5925873147938746\n",
      "0.5916854736770757\n",
      "0.35581372998014305\n",
      "0.5020208282708487\n",
      "0.4212979895084352\n",
      "0.5053213976608395\n",
      "0.6592950207472015\n",
      "0.5842143019787953\n",
      "0.5850735065802161\n",
      "0.5751172801933679\n",
      "0.5880223038926626\n",
      "0.6335114331001057\n",
      "0.6145632747195603\n",
      "0.6821026282573709\n",
      "0.6065078628008413\n",
      "0.6402392261227557\n",
      "0.47970549083935055\n",
      "0.5892252569573686\n",
      "0.4279159892162747\n",
      "0.6314191894697749\n",
      "0.48474950126257255\n",
      "0.3822610272233523\n",
      "0.43252751157887837\n",
      "0.12073932183819705\n",
      "0.4487790216723671\n",
      "0.39139428758276557\n",
      "0.45105100664129044\n",
      "0.577544261220905\n",
      "0.5043251665187714\n",
      "0.27138585722798614\n",
      "0.4153783670658964\n",
      "0.4603096517414665\n",
      "0.5421437895547382\n",
      "0.4699736773977873\n",
      "0.30564019150865346\n",
      "0.4826677121643427\n",
      "0.6967754046903156\n",
      "0.3500586438466058\n",
      "0.32745007998038295\n",
      "0.4413264958061848\n",
      "0.47367254955243077\n",
      "0.4035544687056252\n",
      "0.39829577527131466\n",
      "0.44243757242348397\n",
      "0.42541582752100926\n",
      "0.5309752584649677\n",
      "0.4159955309042637\n",
      "0.39504037615947574\n",
      "0.4280499782707371\n",
      "0.31046584167581354\n",
      "0.3951006678355414\n",
      "0.3961174933552619\n",
      "0.4285353302353043\n",
      "0.6370232615889747\n",
      "0.2095370092267806\n",
      "0.4086326899796889\n",
      "0.3657640458903001\n",
      "0.2855821315530518\n",
      "0.24574468932869573\n",
      "0.35651779854720367\n",
      "0.44210315568791925\n"
     ]
    }
   ],
   "source": [
    "for i in range(82):\n",
    "    print(np.corrcoef(output[:,i].flatten(), label[:,i].flatten())[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dingzhengyao/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Choose the `slow_r50` model \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.models.resnet\n",
    "import torch.nn as nn\n",
    "model = pytorchvideo.models.resnet.create_resnet(\n",
    "      input_channel=1, # RGB input from Kinetics\n",
    "      model_depth=50, # For the tutorial let's just use a 50 layer network\n",
    "      model_num_class=82, # Kinetics has 400 classes so we need out final head to align\n",
    "      norm=nn.BatchNorm3d,\n",
    "      activation=nn.ReLU,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 82])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.randn(1, 1, 50, 224, 224)\n",
    "out = model(data)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Got 4D input, but trilinear mode needs 5D input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 使用插值将图像放大到 224x224\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 输出结果张量的形状\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/data/Anaconda3/envs/pl/lib/python3.9/site-packages/torch/nn/functional.py:4037\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 4D input, but linear mode needs 3D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4037\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 4D input, but trilinear mode needs 5D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but linear mode needs 3D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Got 4D input, but trilinear mode needs 5D input"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设 input_tensor 是一个形状为 [batch_size, 3, 120, 120] 的张量\n",
    "# 这里我们创建一个随机的输入张量作为示例\n",
    "batch_size = 8  # 可以根据您的需求调整batch_size\n",
    "input_tensor = torch.randn(batch_size, 3, 120, 120)\n",
    "\n",
    "# 使用插值将图像放大到 224x224\n",
    "output_tensor = F.interpolate(input_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "# 输出结果张量的形状\n",
    "print(output_tensor.shape)  # 应该是 [batch_size, 3, 224, 224]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
